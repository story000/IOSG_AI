#!/usr/bin/env python3
"""
è·å–æŒ‡å®šfeedsçš„æ‰€æœ‰æœªè¯»æ–‡ç« å¹¶ä¿å­˜ä¸ºJSON
"""

from inoreader_client import InoreaderClient
import json
from datetime import datetime, timedelta
import re

def clean_html(html_content):
    """ç®€å•æ¸…ç†HTMLæ ‡ç­¾"""
    if not html_content:
        return ""
    # ç§»é™¤HTMLæ ‡ç­¾
    clean = re.sub(r'<[^>]+>', '', html_content)
    # è§£ç HTMLå®ä½“
    import html
    clean = html.unescape(clean)
    # ç§»é™¤å¤šä½™ç©ºç™½
    clean = re.sub(r'\s+', ' ', clean).strip()
    return clean

def format_article(article):
    """æ ¼å¼åŒ–æ–‡ç« ä¿¡æ¯"""
    # åŸºæœ¬ä¿¡æ¯
    formatted = {
        'id': article.get('id', ''),
        'title': article.get('title', ''),
        'author': article.get('author', ''),
        'published': article.get('published', 0),
        'published_formatted': '',
        'updated': article.get('updated', 0),
        'url': '',
        'feed_title': '',
        'feed_url': '',
        'content': '',
        'content_text': '',
        'categories': article.get('categories', [])
    }
    
    # æ ¼å¼åŒ–å‘å¸ƒæ—¶é—´
    try:
        if formatted['published']:
            dt = datetime.fromtimestamp(formatted['published'])
            formatted['published_formatted'] = dt.strftime('%Y-%m-%d %H:%M:%S')
    except:
        pass
    
    # è·å–æ–‡ç« URL
    canonical = article.get('canonical', [])
    if canonical:
        formatted['url'] = canonical[0].get('href', '')
    
    # è·å–feedä¿¡æ¯
    origin = article.get('origin', {})
    formatted['feed_title'] = origin.get('title', '')
    formatted['feed_url'] = origin.get('streamId', '')
    
    # è·å–å†…å®¹
    summary = article.get('summary', {})
    content = summary.get('content', '')
    formatted['content'] = content
    formatted['content_text'] = clean_html(content)
    
    return formatted

def main():
    import glob
    import os
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨feedsæ–‡ä»¶
    existing_files = glob.glob("crypto_feeds_unread_*.json")
    if existing_files:
        latest_file = max(existing_files, key=os.path.getctime)
        file_time = datetime.fromtimestamp(os.path.getctime(latest_file))
        time_diff = datetime.now() - file_time
        
        print(f"=== å‘ç°å·²å­˜åœ¨çš„feedsæ–‡ä»¶ ===")
        print(f"ğŸ“„ æ–‡ä»¶: {latest_file}")
        print(f"ğŸ•’ åˆ›å»ºæ—¶é—´: {file_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"â° è·ç¦»ç°åœ¨: {time_diff}")

        # å¦‚æœæ–‡ä»¶åˆ›å»ºæ—¶é—´åœ¨24å°æ—¶å†…ï¼Œç›´æ¥ä½¿ç”¨ç°æœ‰æ–‡ä»¶
        if time_diff.total_seconds() < 86400:  
            print("âœ… æ–‡ä»¶è¾ƒæ–°ï¼ˆ24å°æ—¶å†…ï¼‰ï¼Œç›´æ¥ä½¿ç”¨ç°æœ‰æ–‡ä»¶ï¼Œè·³è¿‡ä¸‹è½½")
            return
        else:
            print("âš ï¸ æ–‡ä»¶è¾ƒæ—§ï¼Œå°†é‡æ–°ä¸‹è½½æœ€æ–°feeds")
    
    # ç›®æ ‡feeds
    target_feeds = [
        "feed/https://rss.panewslab.com/zh/gtimg/rss",
        "feed/https://www.techflowpost.com/rss.aspx", 
        "feed/https://wublockchain123.substack.com/feed",
        "feed/https://cn.cointelegraph.com/rss"
    ]
    
    feed_names = {
        "feed/https://rss.panewslab.com/zh/gtimg/rss": "PANews",
        "feed/https://www.techflowpost.com/rss.aspx": "TechFlow",
        "feed/https://wublockchain123.substack.com/feed": "Wu Blockchain",
        "feed/https://cn.cointelegraph.com/rss": "Cointelegraphä¸­æ–‡"
    }
    
    CLIENT_ID = "1000001559"
    CLIENT_SECRET = "lDyl2_XuuueJYcFZOwNipRy79_TibMOH"
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = InoreaderClient(CLIENT_ID, CLIENT_SECRET)
    
    if not client.is_authenticated():
        print("âš ï¸ éœ€è¦è®¤è¯ï¼Œè¯·è¿è¡Œ smart_client.py å…ˆè¿›è¡Œè®¤è¯")
        return
    
    print("=== è·å–æŒ‡å®šFeedsçš„æœªè¯»æ–‡ç«  ===")
    print(f"ç›®æ ‡feeds: {len(target_feeds)} ä¸ª")
    for feed_id in target_feeds:
        feed_name = feed_names.get(feed_id, feed_id)
        print(f"  ğŸ“° {feed_name}")
    
    all_articles = []
    feed_stats = {}
    
    for feed_id in target_feeds:
        feed_name = feed_names.get(feed_id, feed_id)
        print(f"\næ­£åœ¨è·å– {feed_name} çš„æœªè¯»æ–‡ç« ...")
        
        try:
            # è·å–è¯¥feedçš„æ‰€æœ‰æœªè¯»æ–‡ç« 
            result = client.get_unread_articles(
                stream_id=feed_id,
                max_articles=None,  # è·å–æ‰€æœ‰æœªè¯»æ–‡ç« 
                include_read=False
            )
            
            articles = result['articles']
            count = len(articles)
            
            print(f"âœ“ {feed_name}: æ‰¾åˆ° {count} ç¯‡æœªè¯»æ–‡ç« ")
            
            # æ ¼å¼åŒ–æ–‡ç« å¹¶æ·»åŠ feedæ ‡è¯†
            for article in articles:
                formatted_article = format_article(article)
                formatted_article['source_feed'] = feed_name
                formatted_article['source_feed_id'] = feed_id
                all_articles.append(formatted_article)
            
            feed_stats[feed_name] = count
            
        except Exception as e:
            print(f"âœ— {feed_name}: è·å–å¤±è´¥ - {e}")
            feed_stats[feed_name] = 0
    
    # ç»Ÿè®¡ç»“æœ
    total_articles = len(all_articles)
    print(f"\n=== è·å–å®Œæˆ ===")
    print(f"æ€»è®¡: {total_articles} ç¯‡æœªè¯»æ–‡ç« ")
    
    for feed_name, count in feed_stats.items():
        print(f"  ğŸ“° {feed_name}: {count} ç¯‡")
    
    if total_articles == 0:
        print("ğŸ‰ æ‰€æœ‰æŒ‡å®šfeedséƒ½æ²¡æœ‰æœªè¯»æ–‡ç« ï¼")
        return
    
    # è¿‡æ»¤è¿‘7å¤©å†…çš„æ–‡ç« 
    seven_days_ago = datetime.now() - timedelta(days=7)
    seven_days_timestamp = seven_days_ago.timestamp()
    
    print(f"\n=== åº”ç”¨7å¤©æ—¶é—´è¿‡æ»¤ ===")
    print(f"è¿‡æ»¤åŸºå‡†æ—¶é—´: {seven_days_ago.strftime('%Y-%m-%d %H:%M:%S')}")
    
    original_count = len(all_articles)
    all_articles = [article for article in all_articles if article.get('published', 0) >= seven_days_timestamp]
    filtered_count = len(all_articles)
    
    print(f"åŸå§‹æ–‡ç« æ•°: {original_count}")
    print(f"è¿‡æ»¤åæ–‡ç« æ•°: {filtered_count}")
    print(f"è¿‡æ»¤æ‰: {original_count - filtered_count} ç¯‡ï¼ˆè¶…è¿‡7å¤©ï¼‰")
    
    # æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    all_articles.sort(key=lambda x: x.get('published', 0), reverse=True)
    
    # æ˜¾ç¤ºæœ€æ–°çš„å‡ ç¯‡æ–‡ç« 
    print(f"\n=== æœ€æ–° {min(5, filtered_count)} ç¯‡æ–‡ç« é¢„è§ˆ ===")
    for i, article in enumerate(all_articles[:5], 1):
        print(f"\n{i}. ã€{article['source_feed']}ã€‘{article['title']}")
        print(f"   ä½œè€…: {article['author'] or 'æœªçŸ¥'}")
        print(f"   æ—¶é—´: {article['published_formatted']}")
        print(f"   é“¾æ¥: {article['url']}")
        if article['content_text']:
            preview = article['content_text'][:100] + "..." if len(article['content_text']) > 100 else article['content_text']
            print(f"   é¢„è§ˆ: {preview}")
    
    # ä¿å­˜åˆ°å›ºå®šçš„JSONæ–‡ä»¶
    latest_filename = "latest_feeds.json"
    historical_filename = "historical_feeds.json"
    
    # å‡†å¤‡ä¿å­˜çš„æ•°æ®
    save_data = {
        'metadata': {
            'generated_at': datetime.now().isoformat(),
            'total_articles': filtered_count,
            'original_total': original_count,
            'filtered_out': original_count - filtered_count,
            'filter_days': 7,
            'filter_cutoff': seven_days_ago.isoformat(),
            'feeds_stats': feed_stats,
            'target_feeds': list(feed_names.values())
        },
        'articles': all_articles
    }
    
    # å¦‚æœå­˜åœ¨æ—§çš„æœ€æ–°æ–‡ä»¶ï¼Œå°†å…¶åˆå¹¶åˆ°å†å²æ–‡ä»¶ä¸­
    if os.path.exists(latest_filename):
        try:
            with open(latest_filename, 'r', encoding='utf-8') as f:
                old_latest_data = json.load(f)
            
            # è¯»å–æˆ–åˆ›å»ºå†å²æ–‡ä»¶
            historical_data = {'batches': []}
            if os.path.exists(historical_filename):
                try:
                    with open(historical_filename, 'r', encoding='utf-8') as f:
                        historical_data = json.load(f)
                except:
                    print("âš ï¸ å†å²æ–‡ä»¶è¯»å–å¤±è´¥ï¼Œå°†åˆ›å»ºæ–°çš„å†å²æ–‡ä»¶")
                    historical_data = {'batches': []}
            
            # å°†æ—§çš„æœ€æ–°æ•°æ®æ·»åŠ åˆ°å†å²æ•°æ®ä¸­
            batch_info = {
                'generated_at': old_latest_data['metadata']['generated_at'],
                'total_articles': old_latest_data['metadata']['total_articles'],
                'feeds_stats': old_latest_data['metadata']['feeds_stats']
            }
            historical_data['batches'].append(batch_info)
            
            # ä¿æŒå†å²è®°å½•åœ¨åˆç†æ•°é‡ï¼ˆæœ€å¤šä¿ç•™50ä¸ªæ‰¹æ¬¡ï¼‰
            if len(historical_data['batches']) > 50:
                historical_data['batches'] = historical_data['batches'][-50:]
            
            # æ›´æ–°å†å²æ–‡ä»¶
            historical_data['last_updated'] = datetime.now().isoformat()
            historical_data['total_batches'] = len(historical_data['batches'])
            
            with open(historical_filename, 'w', encoding='utf-8') as f:
                json.dump(historical_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“š æ—§æ•°æ®å·²å½’æ¡£åˆ°å†å²æ–‡ä»¶ ({len(historical_data['batches'])} ä¸ªæ‰¹æ¬¡)")
            
        except Exception as e:
            print(f"âš ï¸ å½’æ¡£æ—§æ•°æ®æ—¶å‡ºé”™: {e}")
    
    # ä¿å­˜æ–°çš„æœ€æ–°æ•°æ®
    with open(latest_filename, 'w', encoding='utf-8') as f:
        json.dump(save_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ æœ€æ–°æ•°æ®å·²ä¿å­˜åˆ°: {latest_filename}")
    print(f"ğŸ“Š æ–‡ä»¶åŒ…å« {filtered_count} ç¯‡æ–‡ç« çš„å®Œæ•´ä¿¡æ¯ï¼ˆè¿‘7å¤©å†…ï¼‰")
    
    # è¯¢é—®æ˜¯å¦æ ‡è®°ä¸ºå·²è¯»
    if filtered_count > 0:
        print(f"\nâ“ æ˜¯å¦å°†è¿™ {filtered_count} ç¯‡æ–‡ç« æ ‡è®°ä¸ºå·²è¯»ï¼Ÿ")
        choice = input("è¾“å…¥ 'y' ç¡®è®¤ï¼Œå…¶ä»–é”®è·³è¿‡: ").lower().strip()
        
        if choice == 'y':
            print("æ­£åœ¨æ ‡è®°æ–‡ç« ä¸ºå·²è¯»...")
            try:
                # é€ä¸ªæ ‡è®°ä¸ºå·²è¯»
                success_count = 0
                for article in all_articles:
                    try:
                        client.mark_article_as_read(article['id'])
                        success_count += 1
                    except Exception as e:
                        print(f"æ ‡è®°å¤±è´¥: {article['title']} - {e}")
                
                print(f"âœ“ æˆåŠŸæ ‡è®° {success_count}/{filtered_count} ç¯‡æ–‡ç« ä¸ºå·²è¯»")
                
            except Exception as e:
                print(f"âœ— æ‰¹é‡æ ‡è®°å¤±è´¥: {e}")
        else:
            print("è·³è¿‡æ ‡è®°å·²è¯»")
    
    print("\nğŸ¯ ä»»åŠ¡å®Œæˆï¼")

if __name__ == "__main__":
    main()