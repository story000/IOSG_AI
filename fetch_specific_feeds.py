#!/usr/bin/env python3
"""
è·å–æŒ‡å®šfeedsçš„æ‰€æœ‰æœªè¯»æ–‡ç« å¹¶ä¿å­˜ä¸ºJSON
"""

from inoreader_client import InoreaderClient
import json
from datetime import datetime, timedelta
import re

def clean_html(html_content):
    """ç®€å•æ¸…ç†HTMLæ ‡ç­¾"""
    if not html_content:
        return ""
    # ç§»é™¤HTMLæ ‡ç­¾
    clean = re.sub(r'<[^>]+>', '', html_content)
    # è§£ç HTMLå®ä½“
    import html
    clean = html.unescape(clean)
    # ç§»é™¤å¤šä½™ç©ºç™½
    clean = re.sub(r'\s+', ' ', clean).strip()
    return clean

def format_article(article):
    """æ ¼å¼åŒ–æ–‡ç« ä¿¡æ¯"""
    # åŸºæœ¬ä¿¡æ¯
    formatted = {
        'id': article.get('id', ''),
        'title': article.get('title', ''),
        'author': article.get('author', ''),
        'published': article.get('published', 0),
        'published_formatted': '',
        'updated': article.get('updated', 0),
        'url': '',
        'feed_title': '',
        'feed_url': '',
        'content': '',
        'content_text': '',
        'categories': article.get('categories', [])
    }
    
    # æ ¼å¼åŒ–å‘å¸ƒæ—¶é—´
    try:
        if formatted['published']:
            dt = datetime.fromtimestamp(formatted['published'])
            formatted['published_formatted'] = dt.strftime('%Y-%m-%d %H:%M:%S')
    except:
        pass
    
    # è·å–æ–‡ç« URL
    canonical = article.get('canonical', [])
    if canonical:
        formatted['url'] = canonical[0].get('href', '')
    
    # è·å–feedä¿¡æ¯
    origin = article.get('origin', {})
    formatted['feed_title'] = origin.get('title', '')
    formatted['feed_url'] = origin.get('streamId', '')
    
    # è·å–å†…å®¹
    summary = article.get('summary', {})
    content = summary.get('content', '')
    formatted['content'] = content
    formatted['content_text'] = clean_html(content)
    
    return formatted

def list_all_feeds(client):
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„è®¢é˜…feeds"""
    try:
        print("\n=== æ­£åœ¨è·å–æ‰€æœ‰è®¢é˜…feeds ===")
        subscription_list = client.get_subscription_list()
        subscriptions = subscription_list.get('subscriptions', [])
        
        if not subscriptions:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•è®¢é˜…feeds")
            return
        
        print(f"ğŸ“‹ æ‰¾åˆ° {len(subscriptions)} ä¸ªè®¢é˜…feeds:")
        print("-" * 80)
        
        for i, sub in enumerate(subscriptions, 1):
            feed_id = sub.get('id', '')
            title = sub.get('title', 'æœªçŸ¥æ ‡é¢˜')
            html_url = sub.get('htmlUrl', '')
            categories = sub.get('categories', [])
            
            print(f"{i:3d}. ğŸ“° {title}")
            print(f"     ğŸ”— Feed ID: {feed_id}")
            if html_url:
                print(f"     ğŸŒ ç½‘ç«™: {html_url}")
            if categories:
                category_names = [cat.get('label', '') for cat in categories]
                print(f"     ğŸ“ åˆ†ç±»: {', '.join(category_names)}")
            print()
        
        print("-" * 80)
        
        # æ˜¾ç¤ºç›®æ ‡feedsçš„åŒ¹é…æƒ…å†µ
        target_feeds_config = {
            "feed/https://rss.panewslab.com/zh/gtimg/rss": "PANews",
            "feed/https://www.techflowpost.com/rss.aspx": "TechFlow",
            "feed/https://wublockchain123.substack.com/feed": "Wu Blockchain",
            "feed/https://cn.cointelegraph.com/rss": "Cointelegraphä¸­æ–‡"
        }
        
        print("ğŸ¯ ç›®æ ‡feedsåŒ¹é…æ£€æŸ¥:")
        subscription_ids = [sub.get('id', '') for sub in subscriptions]
        
        for feed_id, feed_name in target_feeds_config.items():
            if feed_id in subscription_ids:
                print(f"  âœ… {feed_name} - å·²è®¢é˜…")
            else:
                print(f"  âŒ {feed_name} - æœªè®¢é˜…")
        
        return True
        
    except Exception as e:
        print(f"âŒ è·å–feedsåˆ—è¡¨å¤±è´¥: {e}")
        return False

def main():
    import glob
    import os
    import sys
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1 and sys.argv[1] == '--list-feeds':
        # åªåˆ—å‡ºæ‰€æœ‰feedsï¼Œä¸æ‰§è¡Œä¸»è¦é€»è¾‘
        CLIENT_ID = "1000001559"
        CLIENT_SECRET = "lDyl2_XuuueJYcFZOwNipRy79_TibMOH"
        
        client = InoreaderClient(CLIENT_ID, CLIENT_SECRET)
        
        if not client.is_authenticated():
            print("âš ï¸ éœ€è¦è®¤è¯ï¼Œè¯·è¿è¡Œ smart_client.py å…ˆè¿›è¡Œè®¤è¯")
            return
        
        list_all_feeds(client)
        return
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨feedsæ–‡ä»¶
    existing_files = glob.glob("latest_feeds.json")
    if existing_files:
        latest_file = max(existing_files, key=os.path.getctime)
        file_time = datetime.fromtimestamp(os.path.getctime(latest_file))
        time_diff = datetime.now() - file_time
        
        print(f"=== å‘ç°å·²å­˜åœ¨çš„feedsæ–‡ä»¶ ===")
        print(f"ğŸ“„ æ–‡ä»¶: {latest_file}")
        print(f"ğŸ•’ åˆ›å»ºæ—¶é—´: {file_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"â° è·ç¦»ç°åœ¨: {time_diff}")

        # å¦‚æœæ–‡ä»¶åˆ›å»ºæ—¶é—´åœ¨24å°æ—¶å†…ï¼Œç›´æ¥ä½¿ç”¨ç°æœ‰æ–‡ä»¶
        if time_diff.total_seconds() < 86400:  
            print("âœ… æ–‡ä»¶è¾ƒæ–°ï¼ˆ24å°æ—¶å†…ï¼‰ï¼Œç›´æ¥ä½¿ç”¨ç°æœ‰æ–‡ä»¶ï¼Œè·³è¿‡ä¸‹è½½")
            return
        else:
            print("âš ï¸ æ–‡ä»¶è¾ƒæ—§ï¼Œå°†é‡æ–°ä¸‹è½½æœ€æ–°feeds")
    
    # ç›®æ ‡feedsé…ç½®ï¼ˆfeed_id -> feed_name æ˜ å°„ï¼‰
    target_feeds_config = {
        "feed/https://rss.panewslab.com/zh/gtimg/rss": "PANews",
        "feed/https://www.techflowpost.com/rss.aspx": "TechFlow",
        "feed/https://wublockchain123.substack.com/feed": "Wu Blockchain",
        "feed/https://cn.cointelegraph.com/rss": "Cointelegraphä¸­æ–‡",
        "feed/https://substack.chainfeeds.xyz/feed": "ChainFeeds",
        "feed/https://api.theblockbeats.news/v1/open-api/home-xml": "The BlockBeats",
        "feed/https://www.odaily.news/v1/openapi/odailyrss": "Odaily",
        "feed/https://www.coindesk.com/arc/outboundfeeds/rss/?outputType=xml": "Coindesk"
    }
    
    CLIENT_ID = "1000001559"
    CLIENT_SECRET = "lDyl2_XuuueJYcFZOwNipRy79_TibMOH"
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = InoreaderClient(CLIENT_ID, CLIENT_SECRET)
    
    if not client.is_authenticated():
        print("âš ï¸ éœ€è¦è®¤è¯ï¼Œè¯·è¿è¡Œ smart_client.py å…ˆè¿›è¡Œè®¤è¯")
        return
    
    # å…ˆåˆ—å‡ºæ‰€æœ‰feedsä»¥ä¾¿è°ƒè¯•
    # print("=== è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥feedsè®¢é˜…çŠ¶æ€ ===")
    # list_all_feeds(client)
    
    print("\n=== è·å–æŒ‡å®šFeedsçš„æœªè¯»æ–‡ç«  ===")
    print(f"ç›®æ ‡feeds: {len(target_feeds_config)} ä¸ª")
    
    for feed_id, feed_name in target_feeds_config.items():
        print(f"  ğŸ“° {feed_name}")
    
    all_articles = []
    feed_stats = {}
    
    for feed_id, feed_name in target_feeds_config.items():
        print(f"\næ­£åœ¨è·å– {feed_name} çš„æœªè¯»æ–‡ç« ...")
        
        try:
            # è·å–è¯¥feedçš„æ‰€æœ‰æœªè¯»æ–‡ç« 
            result = client.get_unread_articles(
                stream_id=feed_id,
                max_articles=None,  # è·å–æ‰€æœ‰æœªè¯»æ–‡ç« 
                include_read=False
            )
            
            articles = result['articles']
            count = len(articles)
            
            print(f"âœ“ {feed_name}: æ‰¾åˆ° {count} ç¯‡æœªè¯»æ–‡ç« ")
            
            # æ ¼å¼åŒ–æ–‡ç« å¹¶æ·»åŠ feedæ ‡è¯†
            for article in articles:
                formatted_article = format_article(article)
                formatted_article['source_feed'] = feed_name
                formatted_article['source_feed_id'] = feed_id
                all_articles.append(formatted_article)
            
            feed_stats[feed_name] = count
            
        except Exception as e:
            print(f"âœ— {feed_name}: è·å–å¤±è´¥ - {e}")
            feed_stats[feed_name] = 0
    
    # ç»Ÿè®¡ç»“æœ
    total_articles = len(all_articles)
    print(f"\n=== è·å–å®Œæˆ ===")
    print(f"æ€»è®¡: {total_articles} ç¯‡æœªè¯»æ–‡ç« ")
    
    for feed_name, count in feed_stats.items():
        print(f"  ğŸ“° {feed_name}: {count} ç¯‡")
    
    if total_articles == 0:
        print("ğŸ‰ æ‰€æœ‰æŒ‡å®šfeedséƒ½æ²¡æœ‰æœªè¯»æ–‡ç« ï¼")
        return
    
    # è¿‡æ»¤è¿‘7å¤©å†…çš„æ–‡ç« 
    seven_days_ago = datetime.now() - timedelta(days=7)
    seven_days_timestamp = seven_days_ago.timestamp()
    
    print(f"\n=== åº”ç”¨7å¤©æ—¶é—´è¿‡æ»¤ ===")
    print(f"è¿‡æ»¤åŸºå‡†æ—¶é—´: {seven_days_ago.strftime('%Y-%m-%d %H:%M:%S')}")
    
    original_count = len(all_articles)
    all_articles = [article for article in all_articles if article.get('published', 0) >= seven_days_timestamp]
    filtered_count = len(all_articles)
    
    print(f"åŸå§‹æ–‡ç« æ•°: {original_count}")
    print(f"è¿‡æ»¤åæ–‡ç« æ•°: {filtered_count}")
    print(f"è¿‡æ»¤æ‰: {original_count - filtered_count} ç¯‡ï¼ˆè¶…è¿‡7å¤©ï¼‰")
    
    # æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    all_articles.sort(key=lambda x: x.get('published', 0), reverse=True)
    
    # æ˜¾ç¤ºæœ€æ–°çš„å‡ ç¯‡æ–‡ç« 
    print(f"\n=== æœ€æ–° {min(5, filtered_count)} ç¯‡æ–‡ç« é¢„è§ˆ ===")
    for i, article in enumerate(all_articles[:5], 1):
        print(f"\n{i}. ã€{article['source_feed']}ã€‘{article['title']}")
        print(f"   ä½œè€…: {article['author'] or 'æœªçŸ¥'}")
        print(f"   æ—¶é—´: {article['published_formatted']}")
        print(f"   é“¾æ¥: {article['url']}")
        if article['content_text']:
            preview = article['content_text'][:100] + "..." if len(article['content_text']) > 100 else article['content_text']
            print(f"   é¢„è§ˆ: {preview}")
    
    # ä¿å­˜åˆ°å›ºå®šçš„JSONæ–‡ä»¶
    latest_filename = "latest_feeds.json"
    historical_filename = "historical_feeds.json"
    
    # å‡†å¤‡ä¿å­˜çš„æ•°æ®
    save_data = {
        'metadata': {
            'generated_at': datetime.now().isoformat(),
            'total_articles': filtered_count,
            'original_total': original_count,
            'filtered_out': original_count - filtered_count,
            'filter_days': 7,
            'filter_cutoff': seven_days_ago.isoformat(),
            'feeds_stats': feed_stats,
            'target_feeds': list(target_feeds_config.values())
        },
        'articles': all_articles
    }
    
    # å¦‚æœå­˜åœ¨æ—§çš„æœ€æ–°æ–‡ä»¶ï¼Œå°†å…¶å®Œæ•´æ•°æ®åˆå¹¶åˆ°å†å²æ–‡ä»¶ä¸­
    if os.path.exists(latest_filename):
        try:
            with open(latest_filename, 'r', encoding='utf-8') as f:
                old_latest_data = json.load(f)
            
            # è¯»å–æˆ–åˆ›å»ºå†å²æ–‡ä»¶
            historical_data = {'all_articles': [], 'batches_metadata': []}
            if os.path.exists(historical_filename):
                try:
                    with open(historical_filename, 'r', encoding='utf-8') as f:
                        loaded_data = json.load(f)
                    
                    # æ£€æŸ¥æ–‡ä»¶æ ¼å¼å…¼å®¹æ€§
                    if 'all_articles' in loaded_data:
                        # æ–°æ ¼å¼ï¼ˆä¿å­˜æ‰€æœ‰æ–‡ç« ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
                        historical_data = loaded_data
                    elif 'batches' in loaded_data:
                        # ä¸­é—´æ ¼å¼ï¼ˆåªä¿å­˜æ‘˜è¦ï¼‰ï¼Œè½¬æ¢ä¸ºæ–°æ ¼å¼
                        print("ğŸ”„ æ£€æµ‹åˆ°æ‰¹æ¬¡æ‘˜è¦æ ¼å¼ï¼Œè½¬æ¢ä¸ºå®Œæ•´æ–‡ç« å­˜å‚¨æ ¼å¼...")
                        historical_data = {'all_articles': [], 'batches_metadata': loaded_data.get('batches', [])}
                    elif 'metadata' in loaded_data and 'articles' in loaded_data:
                        # æ—§æ ¼å¼ï¼ˆå•æ‰¹æ¬¡å®Œæ•´æ•°æ®ï¼‰ï¼Œè½¬æ¢ä¸ºæ–°æ ¼å¼
                        print("ğŸ”„ æ£€æµ‹åˆ°æ—§æ ¼å¼å†å²æ–‡ä»¶ï¼Œæ­£åœ¨è½¬æ¢...")
                        historical_data = {
                            'all_articles': loaded_data['articles'],
                            'batches_metadata': [{
                                'generated_at': loaded_data['metadata']['generated_at'],
                                'total_articles': loaded_data['metadata']['total_articles'],
                                'feeds_stats': loaded_data['metadata'].get('feeds_stats', {})
                            }]
                        }
                    else:
                        # æœªçŸ¥æ ¼å¼ï¼Œåˆ›å»ºæ–°çš„
                        print("âš ï¸ å†å²æ–‡ä»¶æ ¼å¼æœªçŸ¥ï¼Œå°†åˆ›å»ºæ–°çš„å†å²æ–‡ä»¶")
                        historical_data = {'all_articles': [], 'batches_metadata': []}
                        
                except Exception as e:
                    print(f"âš ï¸ å†å²æ–‡ä»¶è¯»å–å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°çš„å†å²æ–‡ä»¶")
                    historical_data = {'all_articles': [], 'batches_metadata': []}
            
            # å°†æ—§çš„æœ€æ–°æ•°æ®çš„æ‰€æœ‰æ–‡ç« æ·»åŠ åˆ°å†å²æ•°æ®ä¸­
            old_articles = old_latest_data.get('articles', [])
            if old_articles:
                # ç»™æ¯ä¸ªæ–‡ç« æ·»åŠ æ‰¹æ¬¡æ ‡è¯†
                batch_id = old_latest_data['metadata']['generated_at']
                for article in old_articles:
                    article['batch_id'] = batch_id
                
                # åˆå¹¶åˆ°å†å²æ–‡ç« åˆ—è¡¨
                historical_data['all_articles'].extend(old_articles)
                
                # æ·»åŠ æ‰¹æ¬¡å…ƒæ•°æ®
                batch_metadata = {
                    'generated_at': old_latest_data['metadata']['generated_at'],
                    'total_articles': old_latest_data['metadata']['total_articles'],
                    'feeds_stats': old_latest_data['metadata']['feeds_stats']
                }
                historical_data['batches_metadata'].append(batch_metadata)
                
                print(f"ğŸ“š å·²å°† {len(old_articles)} ç¯‡æ–‡ç« å½’æ¡£åˆ°å†å²æ–‡ä»¶")
            
            # æ›´æ–°å†å²æ–‡ä»¶å…ƒæ•°æ®
            historical_data['last_updated'] = datetime.now().isoformat()
            historical_data['total_articles'] = len(historical_data['all_articles'])
            historical_data['total_batches'] = len(historical_data['batches_metadata'])
            
            with open(historical_filename, 'w', encoding='utf-8') as f:
                json.dump(historical_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“š å†å²æ–‡ä»¶å·²æ›´æ–°ï¼Œå…±åŒ…å« {historical_data['total_articles']} ç¯‡æ–‡ç« ï¼Œ{historical_data['total_batches']} ä¸ªæ‰¹æ¬¡")
            
        except Exception as e:
            print(f"âš ï¸ å½’æ¡£æ—§æ•°æ®æ—¶å‡ºé”™: {e}")
    
    # ä¿å­˜æ–°çš„æœ€æ–°æ•°æ®
    with open(latest_filename, 'w', encoding='utf-8') as f:
        json.dump(save_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ æœ€æ–°æ•°æ®å·²ä¿å­˜åˆ°: {latest_filename}")
    print(f"ğŸ“Š æ–‡ä»¶åŒ…å« {filtered_count} ç¯‡æ–‡ç« çš„å®Œæ•´ä¿¡æ¯ï¼ˆè¿‘7å¤©å†…ï¼‰")
    
    # è¯¢é—®æ˜¯å¦æ ‡è®°ä¸ºå·²è¯»
    if filtered_count > 0:
        print(f"\nâ“ æ˜¯å¦å°†è¿™ {filtered_count} ç¯‡æ–‡ç« æ ‡è®°ä¸ºå·²è¯»ï¼Ÿ")
        choice = input("è¾“å…¥ 'y' ç¡®è®¤ï¼Œå…¶ä»–é”®è·³è¿‡: ").lower().strip()
        
        if choice == 'y':
            print("æ­£åœ¨æ ‡è®°æ–‡ç« ä¸ºå·²è¯»...")
            try:
                # é€ä¸ªæ ‡è®°ä¸ºå·²è¯»
                success_count = 0
                for article in all_articles:
                    try:
                        client.mark_article_as_read(article['id'])
                        success_count += 1
                    except Exception as e:
                        print(f"æ ‡è®°å¤±è´¥: {article['title']} - {e}")
                
                print(f"âœ“ æˆåŠŸæ ‡è®° {success_count}/{filtered_count} ç¯‡æ–‡ç« ä¸ºå·²è¯»")
                
            except Exception as e:
                print(f"âœ— æ‰¹é‡æ ‡è®°å¤±è´¥: {e}")
        else:
            print("è·³è¿‡æ ‡è®°å·²è¯»")
    
    print("\nğŸ¯ ä»»åŠ¡å®Œæˆï¼")

if __name__ == "__main__":
    main()