#!/usr/bin/env python3
"""
åŸºäºAI APIçš„é¡¹ç›®èèµ„æ–‡ç« ç­›é€‰å™¨ (æ”¯æŒOpenAIå’ŒDeepSeek)
ä»classify_articles.pyçš„ç»“æœä¸­è¿›ä¸€æ­¥ç­›é€‰çœŸå®çš„é¡¹ç›®èèµ„
"""

import json
import openai
import time
from datetime import datetime
import os
import re
import yaml
import difflib
import time

class AIFundingFilter:
    # ç±»åˆ«åç§°æ˜ å°„ - å®šä¹‰ä¸ºç±»å¸¸é‡ï¼Œé¿å…ä»£ç é‡å¤
    TYPE_NAME_MAPPING = {
        "project": "é¡¹ç›®èèµ„",
        "fund": "åŸºé‡‘èèµ„",
        "blockchain": "å…¬é“¾/L2/ä¸»ç½‘",
        "middleware": "ä¸­é—´ä»¶/å·¥å…·åè®®",
        "defi": "DeFi",
        "rwa": "RWA",
        "stablecoin": "ç¨³å®šå¸",
        "application": "åº”ç”¨åè®®",
        "gamefi": "GameFi",
        "exchange_wallet": "äº¤æ˜“æ‰€/é’±åŒ…",
        "ai_crypto": "AI + Crypto",
        "depin": "DePIN",
        "portfolio": "Portfolio"
    }
    
    def __init__(self, api_key=None, provider="openai"):
        self.provider = provider.lower()
        self.api_key = api_key

        # åå°OpenAI APIå¯†é’¥ (è¯·åœ¨è¿™é‡Œé…ç½®æ‚¨çš„OpenAI APIå¯†é’¥)
        # æ–¹å¼1: ç›´æ¥åœ¨ä»£ç ä¸­é…ç½®
        # self.deepseek_key = "sk-your-actual-deepseek-key-here"  
        # æ–¹å¼2: ä»ç¯å¢ƒå˜é‡è¯»å–
        self.deepseek_key = os.getenv('DEEPSEEK_API_KEY', 'sk-your-deepseek-key-here')
        
        if self.provider == "deepseek":
            # ä½¿ç”¨DeepSeek API
            if not self.deepseek_key or self.deepseek_key == 'sk-your-deepseek-key-here':
                print("âŒ DeepSeek APIå¯†é’¥æœªé…ç½®")
                print("è¯·åœ¨ai_filter.pyç¬¬21è¡Œé…ç½®æ‚¨çš„DeepSeek APIå¯†é’¥:")
                print('self.deepseek_key = "sk-your-actual-deepseek-key-here"')
                print("æˆ–è®¾ç½®ç¯å¢ƒå˜é‡: export DEEPSEEK_API_KEY=sk-your-key")
                self.api_key = None
                return
            self.api_key = self.deepseek_key
            self.base_url = "https://api.deepseek.com"
            self.model = "deepseek-chat"
            print("âœ… ä½¿ç”¨DeepSeek API")
            print(f"APIå¯†é’¥: {self.api_key[:10]}...")
        else:
            # ä½¿ç”¨OpenAI API
            if api_key:
                self.api_key = api_key
            else:
                self.api_key = os.getenv('OPENAI_API_KEY')
            if not self.api_key:
                print("âŒ è¯·æä¾›OpenAI APIå¯†é’¥")
                return
            self.base_url = None  # OpenAIé»˜è®¤
            self.model = "gpt-4.1"
            print("âœ… ä½¿ç”¨OpenAI API", f"æ¨¡å‹: {self.model}")
        
        # è®¾ç½®OpenAIå®¢æˆ·ç«¯
        if self.provider == "deepseek":
            openai.api_key = self.api_key
            openai.base_url = self.base_url
        else:
            openai.api_key = self.api_key
        
        
        # ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®
        self.config = self._load_config()
        self.portfolios = self.config.get('portfolio_projects', [])
        self.prompts_config = self.config.get('ai_filter', {})
        
        # æ ‡é¢˜å»é‡ç›¸ä¼¼åº¦é˜ˆå€¼
        self.similarity_threshold = 0.7

    def _load_config(self):
        """ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®"""
        try:
            config_file = "crypto_config.yaml"
            with open(config_file, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            print(f"âœ… å·²åŠ è½½é…ç½®æ–‡ä»¶: {config_file}")
            return config
        except FileNotFoundError:
            print("âŒ æœªæ‰¾åˆ°crypto_config.yamlé…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return {}
        except Exception as e:
            print(f"âŒ åŠ è½½é…ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return {}

    def _format_criteria(self, criteria):
        """æ ¼å¼åŒ–ç­›é€‰æ ‡å‡†ä¸ºå¯è¯»æ–‡æœ¬"""
        formatted = ""
        if 'keep' in criteria:
            for item in criteria['keep']:
                formatted += f"âœ… ä¿ç•™ï¼š{item}\n"
        if 'discard' in criteria:
            for item in criteria['discard']:
                formatted += f"âŒ æŠ›å¼ƒï¼š{item}\n"
        return formatted.strip()

    def _generate_prompt(self, category_name, count, articles_text):
        """æ ¹æ®ç±»åˆ«ç”Ÿæˆprompt"""
        if not self.prompts_config:
            # ä½¿ç”¨æ—§çš„ç¡¬ç¼–ç æ–¹å¼ä½œä¸ºfallback
            return self._get_legacy_prompt(category_name, count, articles_text)
        
        config = self.prompts_config
        
        # æŸ¥æ‰¾å¯¹åº”çš„ç±»åˆ«é…ç½®
        category_config = None
        for cat_name, cat_config in config['categories'].items():
            if cat_name == category_name:
                category_config = cat_config
                break
        
        if not category_config:
            print(f"âš ï¸ æœªæ‰¾åˆ°ç±»åˆ« '{category_name}' çš„é…ç½®ï¼Œä½¿ç”¨é¡¹ç›®èèµ„é…ç½®")
            category_config = config['categories']['é¡¹ç›®èèµ„']
        
        # æ ¼å¼åŒ–ç­›é€‰æ ‡å‡†
        criteria_text = self._format_criteria(category_config['criteria'])
        
        # ç”Ÿæˆå®Œæ•´çš„prompt
        prompt = category_config['prompt_template'].format(
            count=count,
            criteria=criteria_text,
            return_format=config['common']['return_format'],
            empty_return=config['common']['empty_return'],
            instructions=config['common']['instructions'],
            articles=articles_text
        )
        
        return prompt
    
    def _clean_title_for_comparison(self, title):
        """
        æ¸…ç†æ ‡é¢˜ç”¨äºç›¸ä¼¼åº¦æ¯”è¾ƒ
        """
        # ç§»é™¤å¤šä½™ç©ºæ ¼å’Œæ ‡ç‚¹
        title = re.sub(r'\s+', ' ', title).strip()
        # ç§»é™¤å¸¸è§çš„æ–°é—»å‰ç¼€
        title = re.sub(r'^\d+\.\s*', '', title)
        return title.lower()
    
    def _calculate_title_similarity(self, title1, title2):
        """
        è®¡ç®—ä¸¤ä¸ªæ ‡é¢˜çš„ç›¸ä¼¼åº¦
        ä½¿ç”¨ difflib.SequenceMatcher è®¡ç®—å­—ç¬¦ä¸²ç›¸ä¼¼åº¦
        """
        clean_title1 = self._clean_title_for_comparison(title1)
        clean_title2 = self._clean_title_for_comparison(title2)
        
        similarity = difflib.SequenceMatcher(None, clean_title1, clean_title2).ratio()
        return similarity
    
    def deduplicate_articles_by_title(self, articles, category_name=""):
        """
        æ ¹æ®æ ‡é¢˜å­—ç¬¦ç›¸ä¼¼åº¦å»é‡æ–‡ç« 
        :param articles: æ–‡ç« åˆ—è¡¨
        :param category_name: åˆ†ç±»åç§°ï¼ˆç”¨äºæ—¥å¿—è¾“å‡ºï¼‰
        :return: (å»é‡åçš„æ–‡ç« åˆ—è¡¨, é‡å¤æ–‡ç« ç»Ÿè®¡)
        """
        if len(articles) <= 1:
            return articles, {'removed_count': 0, 'duplicate_groups': []}
        
        # ç”¨äºå­˜å‚¨å»é‡åçš„æ–‡ç« 
        unique_articles = []
        # ç”¨äºå­˜å‚¨é‡å¤æ–‡ç« ç»„
        duplicate_groups = []
        # å·²å¤„ç†çš„æ–‡ç« ç´¢å¼•
        processed = set()
        
        print(f"  ğŸ” å¼€å§‹å¯¹{category_name}è¿›è¡Œå­—ç¬¦ç›¸ä¼¼åº¦å»é‡ï¼ˆé˜ˆå€¼: {self.similarity_threshold:.0%}ï¼‰...")
        
        for i, article1 in enumerate(articles):
            if i in processed:
                continue
                
            # å½“å‰æ–‡ç« ç»„ï¼ˆåŒ…å«ç›¸ä¼¼çš„æ–‡ç« ï¼‰
            current_group = [article1]
            processed.add(i)
            
            # ä¸åç»­æ–‡ç« æ¯”è¾ƒ
            for j, article2 in enumerate(articles[i+1:], i+1):
                if j in processed:
                    continue
                
                # ä½¿ç”¨å­—ç¬¦ç›¸ä¼¼åº¦
                char_similarity = self._calculate_title_similarity(
                    article1['title'], article2['title']
                )
                
                if char_similarity >= self.similarity_threshold:
                    current_group.append(article2)
                    processed.add(j)
                    
            # å¦‚æœæœ‰é‡å¤æ–‡ç« ï¼Œè®°å½•åˆ°é‡å¤ç»„ï¼›å¦åˆ™æ·»åŠ åˆ°å”¯ä¸€æ–‡ç« 
            if len(current_group) > 1:
                duplicate_groups.append(current_group)
                # ä¿ç•™ç¬¬ä¸€ç¯‡æ–‡ç« ï¼ˆé€šå¸¸æ˜¯æœ€æ—©å‘å¸ƒçš„ï¼‰
                unique_articles.append(current_group[0])
                
                # è¾“å‡ºé‡å¤ä¿¡æ¯
                print(f"    ğŸ“ å‘ç°é‡å¤ç»„ï¼ˆ{len(current_group)}ç¯‡ï¼‰:")
                for k, article in enumerate(current_group):
                    status = "âœ… ä¿ç•™" if k == 0 else "âŒ åˆ é™¤"
                    title_preview = article['title'][:50] + "..." if len(article['title']) > 50 else article['title']
                    print(f"      {status} {title_preview}")
                    
            else:
                unique_articles.append(current_group[0])
                
        removed_count = len(articles) - len(unique_articles)
        removal_rate = (removed_count / len(articles)) * 100 if len(articles) > 0 else 0
        
        print(f"  ğŸ“Š {category_name}å­—ç¬¦ç›¸ä¼¼åº¦å»é‡å®Œæˆ: {len(articles)} â†’ {len(unique_articles)} ç¯‡ (åˆ é™¤{removed_count}ç¯‡, {removal_rate:.1f}%)")
        
        return unique_articles, {
            'removed_count': removed_count,
            'duplicate_groups': duplicate_groups,
            'removal_rate': removal_rate
        }

    def ai_batch_semantic_deduplication(self, filtered_results):
        """
        AIæ‰¹é‡è¯­ä¹‰å»é‡ï¼šæ”¶é›†æ‰€æœ‰æ–‡ç« æ ‡é¢˜ï¼Œè®©AIä¸€æ¬¡æ€§è¯†åˆ«é‡å¤ç»„
        """
        print(f"\nğŸ¤– å¼€å§‹AIæ‰¹é‡è¯­ä¹‰å»é‡...")
        
        # æ”¶é›†æ‰€æœ‰æ–‡ç« ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
        priority_categories = [
            ("portfolio", "Portfolio"),
            ("project", "é¡¹ç›®èèµ„"), 
            ("fund", "åŸºé‡‘èèµ„"),
            ("blockchain", "å…¬é“¾/L2/ä¸»ç½‘"),
            ("middleware", "ä¸­é—´ä»¶/å·¥å…·åè®®"),
            ("defi", "DeFi"),
            ("rwa", "RWA"),
            ("stablecoin", "ç¨³å®šå¸"),
            ("application", "åº”ç”¨åè®®"),
            ("gamefi", "GameFi"),
            ("exchange_wallet", "äº¤æ˜“æ‰€/é’±åŒ…"),
            ("ai_crypto", "AI + Crypto"),
            ("depin", "DePIN")
        ]
        
        # æ„å»ºå…¨å±€æ–‡ç« åˆ—è¡¨ï¼Œè®°å½•æ¥æº
        all_articles = []
        article_sources = {}  # è®°å½•æ–‡ç« IDåˆ°(category_key, index)çš„æ˜ å°„
        
        global_id = 1
        for category_key, category_name in priority_categories:
            articles = filtered_results.get(category_key, [])
            for i, article in enumerate(articles):
                article_id = f"ID{global_id}"
                all_articles.append({
                    'id': article_id,
                    'title': article['title'],
                    'article': article,
                    'category_key': category_key,
                    'category_name': category_name,
                    'index': i
                })
                article_sources[article_id] = (category_key, i)
                global_id += 1
        
        if len(all_articles) <= 1:
            print("  ğŸ“Š æ–‡ç« æ•°é‡ä¸è¶³ï¼Œæ— éœ€AIå»é‡")
            return filtered_results, {'removed_count': 0, 'duplicate_groups': []}
        
        # è·å–AIè¯­ä¹‰å»é‡é…ç½®
        semantic_dedup_config = self.config.get('ai_semantic_deduplication', {})
        settings = semantic_dedup_config.get('settings', {})
        batch_size_limit = settings.get('batch_size_limit', 100)
        
        # æ˜¾ç¤ºæ–‡ç« æ•°é‡ï¼ˆä½†ä¸é™åˆ¶ï¼‰
        if len(all_articles) > batch_size_limit:
            print(f"  ğŸ“‹ æ–‡ç« æ•°é‡({len(all_articles)})è¶…è¿‡æ¨èé™åˆ¶({batch_size_limit})ï¼Œå°†å¤„ç†å…¨éƒ¨æ–‡ç« ")
        
        print(f"  ğŸ“‹ æ”¶é›†åˆ° {len(all_articles)} ç¯‡æ–‡ç« ï¼Œå‡†å¤‡AIè¯­ä¹‰å»é‡...")
        
        # ä»é…ç½®æ–‡ä»¶ç”ŸæˆAIå»é‡æç¤º
        articles_text = ""
        for item in all_articles:
            articles_text += f"\n{item['id']}: {item['title']} [{item['category_name']}]"
        
        prompt_template = semantic_dedup_config.get('prompt_template', '')
        
        # æ ¼å¼åŒ–prompt
        prompt = prompt_template.format(
            total_articles=len(all_articles),
            articles_text=articles_text
        )

        try:
            # è·å–APIè°ƒç”¨è®¾ç½®
            max_tokens = settings.get('max_tokens', 1000)
            temperature = settings.get('temperature', 0)
            
            # è°ƒç”¨AI API
            if self.provider == "deepseek":
                client = openai.OpenAI(
                    api_key=self.api_key,
                    base_url=self.base_url
                )
            else:
                client = openai.OpenAI(api_key=self.api_key)
                
            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=max_tokens,
                temperature=temperature
            )
            
            result = response.choices[0].message.content.strip()
            print(f"  ğŸ¤– AIè¿”å›: {result}")
            
            # è§£æAIè¿”å›çš„é‡å¤ç»„
            import re
            import ast
            
            # å°è¯•è§£æä¸ºPythonåˆ—è¡¨
            try:
                if result.strip() == "[]":
                    duplicate_groups = []
                else:
                    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–IDç»„åˆï¼Œæ”¯æŒIDæ ¼å¼
                    pattern = r'\[(ID\d+(?:,\s*ID\d+)+)\]'
                    matches = re.findall(pattern, result)
                    duplicate_groups = []
                    
                    if matches:
                        # å¤„ç†IDæ ¼å¼çš„è¿”å›
                        for match in matches:
                            ids = re.findall(r'ID\d+', match)
                            if len(ids) >= 2:
                                duplicate_groups.append(ids)
                    else:
                        # å°è¯•å¤„ç†çº¯æ•°å­—æ ¼å¼çš„è¿”å›
                        pattern = r'\[([0-9,\s]+)\]'
                        matches = re.findall(pattern, result)
                        for match in matches:
                            numbers = re.findall(r'\d+', match)
                            if len(numbers) >= 2:
                                ids = [f"ID{num}" for num in numbers]
                                duplicate_groups.append(ids)
                
            except Exception as e:
                print(f"  âš ï¸ è§£æAIè¿”å›ç»“æœå¤±è´¥: {e}")
                duplicate_groups = []
            
            if not duplicate_groups:
                print("  ğŸ“Š AIæœªå‘ç°é‡å¤æ–‡ç« ")
                return filtered_results, {'removed_count': 0, 'duplicate_groups': []}
            
            # å¤„ç†é‡å¤ç»„ï¼ŒæŒ‰ä¼˜å…ˆçº§ä¿ç•™æ–‡ç« 
            removed_count = 0
            articles_to_remove = set()  # å­˜å‚¨è¦åˆ é™¤çš„æ–‡ç« (category_key, index)
            
            print(f"  ğŸ“ AIå‘ç° {len(duplicate_groups)} ä¸ªé‡å¤ç»„:")
            
            for group_idx, id_group in enumerate(duplicate_groups, 1):
                print(f"    é‡å¤ç»„ {group_idx}: {len(id_group)} ç¯‡æ–‡ç« ")
                
                # è·å–è¿™ç»„æ–‡ç« çš„è¯¦ç»†ä¿¡æ¯
                group_articles = []
                for article_id in id_group:
                    if article_id in article_sources:
                        category_key, index = article_sources[article_id]
                        # æ‰¾åˆ°å¯¹åº”çš„æ–‡ç« 
                        for item in all_articles:
                            if item['id'] == article_id:
                                group_articles.append({
                                    'id': article_id,
                                    'title': item['title'],
                                    'category_key': category_key,
                                    'category_name': item['category_name'],
                                    'index': index,
                                    'priority': next(i for i, (k, _) in enumerate(priority_categories) if k == category_key)
                                })
                                break
                
                # æŒ‰ä¼˜å…ˆçº§æ’åºï¼ˆä¼˜å…ˆçº§æ•°å­—è¶Šå°è¶Šä¼˜å…ˆï¼‰
                group_articles.sort(key=lambda x: x['priority'])
                
                # ä¿ç•™ç¬¬ä¸€ç¯‡ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰ï¼Œåˆ é™¤å…¶ä»–
                keep_article = group_articles[0]
                remove_articles = group_articles[1:]
                
                print(f"      âœ… ä¿ç•™: {keep_article['title'][:50]}... [{keep_article['category_name']}]")
                
                for article in remove_articles:
                    print(f"      âŒ åˆ é™¤: {article['title'][:50]}... [{article['category_name']}]")
                    articles_to_remove.add((article['category_key'], article['index']))
                    removed_count += 1
            
            # ä»ç»“æœä¸­åˆ é™¤é‡å¤æ–‡ç« 
            for category_key, category_name in priority_categories:
                if category_key in filtered_results:
                    original_articles = filtered_results[category_key]
                    # æ„å»ºè¦ä¿ç•™çš„æ–‡ç« åˆ—è¡¨
                    filtered_articles = []
                    for i, article in enumerate(original_articles):
                        if (category_key, i) not in articles_to_remove:
                            filtered_articles.append(article)
                    
                    # æ›´æ–°ç»“æœ
                    if len(filtered_articles) != len(original_articles):
                        removed_from_category = len(original_articles) - len(filtered_articles)
                        print(f"    ğŸ“Š ä»{category_name}åˆ é™¤ {removed_from_category} ç¯‡é‡å¤æ–‡ç« ")
                        filtered_results[category_key] = filtered_articles
            
            print(f"  ğŸ¯ AIæ‰¹é‡è¯­ä¹‰å»é‡å®Œæˆï¼Œå…±åˆ é™¤ {removed_count} ç¯‡é‡å¤æ–‡ç« ")
            
            return filtered_results, {
                'removed_count': removed_count,
                'duplicate_groups': duplicate_groups
            }
            
        except Exception as e:
            print(f"  âŒ AIæ‰¹é‡è¯­ä¹‰å»é‡å¤±è´¥: {e}")
            return filtered_results, {'removed_count': 0, 'duplicate_groups': []}

    def _summarize_report_with_ai(self, article_content):
        """
        ä½¿ç”¨AIæ€»ç»“ç ”æŠ¥å†…å®¹
        """
        # è·å–paragraph_water_infoé…ç½®
        paragraph_config = self.config.get('paragraph_water_info', {})
        prompt_template = paragraph_config.get('prompt_template', '')
        
        if not prompt_template:
            print("  âš ï¸ æœªæ‰¾åˆ°paragraph_water_infoé…ç½®ï¼Œä½¿ç”¨åŸæ–‡")
            return article_content
        
        # æ ¼å¼åŒ–prompt
        prompt = prompt_template.format(article_text=article_content)
        
        try:
            # è°ƒç”¨AI API
            if self.provider == "deepseek":
                client = openai.OpenAI(
                    api_key=self.api_key,
                    base_url=self.base_url
                )
            else:
                client = openai.OpenAI(api_key=self.api_key)
                
            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1500,
                temperature=0.3
            )
            
            summarized = response.choices[0].message.content.strip()
            print(f"  âœ… AIæ€»ç»“å®Œæˆï¼ŒåŸæ–‡{len(article_content)}å­— â†’ æ€»ç»“{len(summarized)}å­—")
            return summarized
            
        except Exception as e:
            print(f"  âŒ AIæ€»ç»“å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸæ–‡")
            return article_content

    def _generate_funding_table_with_ai(self, project_articles):
        """
        ä½¿ç”¨AIç»Ÿä¸€ç”Ÿæˆèèµ„ä¿¡æ¯è¡¨æ ¼ï¼ˆJSONæ ¼å¼è½¬Markdownè¡¨æ ¼ï¼‰
        """
        # åˆå¹¶æ‰€æœ‰èèµ„æ–‡ç« 
        all_funding_articles = project_articles
        
        if not all_funding_articles:
            return ""
        
        # è·å–èèµ„è¡¨æ ¼æå–é…ç½®
        funding_config = self.config.get('funding_table_extraction', {})
        prompt_template = funding_config.get('prompt_template', '')
        
        if not prompt_template:
            print("  âš ï¸ æœªæ‰¾åˆ°funding_table_extractioné…ç½®ï¼Œè·³è¿‡è¡¨æ ¼ç”Ÿæˆ")
            return ""
        
        print(f"  ğŸ“Š æ­£åœ¨ç»Ÿä¸€ç”Ÿæˆèèµ„ä¿¡æ¯è¡¨æ ¼ï¼ˆ{len(all_funding_articles)}æ¡èèµ„æ–°é—»ï¼‰...")
        
        # æ„å»ºæ‰€æœ‰æ–‡ç« çš„å†…å®¹
        articles_content = ""
        for i, article in enumerate(all_funding_articles, 1):
            articles_content += f"\n=== èèµ„æ–°é—» {i} ===\n"
            articles_content += f"æ ‡é¢˜ï¼š{article['title']}\n"
            articles_content += f"å†…å®¹ï¼š{article['content']}\n"
        
        # æ ¼å¼åŒ–prompt
        prompt = prompt_template.format(
            total_articles=len(all_funding_articles),
            articles_content=articles_content
        )
        
        try:
            # è°ƒç”¨AI API
            if self.provider == "deepseek":
                client = openai.OpenAI(
                    api_key=self.api_key,
                    base_url=self.base_url
                )
            else:
                client = openai.OpenAI(api_key=self.api_key)
                
            settings = funding_config.get('settings', {})
            max_tokens = settings.get('max_tokens', 4000)
            temperature = settings.get('temperature', 0)
            
            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=max_tokens,
                temperature=temperature
            )
            
            ai_result = response.choices[0].message.content.strip()
            print(f"  ğŸ¤– AIè¿”å›åŸå§‹æ•°æ®: {ai_result[:200]}...")
            
            # è§£æJSONå¹¶ç”Ÿæˆæ ‡å‡†markdownè¡¨æ ¼
            table_result = self._parse_json_to_markdown_table(ai_result)
            
            print(f"  âœ… èèµ„ä¿¡æ¯è¡¨æ ¼ç”Ÿæˆå®Œæˆ")
            return table_result
                
        except Exception as e:
            print(f"  âŒ ç”Ÿæˆèèµ„è¡¨æ ¼å¤±è´¥: {e}")
            return ""

    def _parse_json_to_markdown_table(self, ai_result):
        """
        è§£æAIè¿”å›çš„JSONæ•°æ®å¹¶ç”Ÿæˆæ ‡å‡†Markdownè¡¨æ ¼
        """
        try:
            import json
            
            # å°è¯•æå–JSONéƒ¨åˆ†
            json_text = ai_result
            if '{' in ai_result:
                # æ‰¾åˆ°ç¬¬ä¸€ä¸ª{å’Œæœ€åä¸€ä¸ª}
                start = ai_result.find('{')
                end = ai_result.rfind('}') + 1
                json_text = ai_result[start:end]
            
            # è§£æJSON
            data = json.loads(json_text)
            funding_list = data.get('funding_list', [])
            
            if not funding_list:
                print("  âš ï¸ æœªæ‰¾åˆ°èèµ„ä¿¡æ¯")
                return ""
            
            # ç”Ÿæˆæ ‡å‡†markdownè¡¨æ ¼
            table_lines = []
            table_lines.append("| å—èµ„æ–¹ | æŠ•èµ„æ–¹ | èµ›é“ | æ¶‰åŠé‡‘é¢ |")
            table_lines.append("|--------|--------|------|----------|")
            
            for item in funding_list:
                company = item.get('company', 'æœªæŠ«éœ²')
                investors = item.get('investors', 'æœªæŠ«éœ²')
                sector = item.get('sector', 'æœªæŠ«éœ²')
                amount = item.get('amount', 'æœªæŠ«éœ²')
                
                # ç¡®ä¿å•å…ƒæ ¼å†…å®¹ä¸åŒ…å«|å­—ç¬¦ï¼Œé¿å…ç ´åè¡¨æ ¼
                company = str(company).replace('|', '/')
                investors = str(investors).replace('|', '/')
                sector = str(sector).replace('|', '/')
                amount = str(amount).replace('|', '/')
                
                table_lines.append(f"| {company} | {investors} | {sector} | {amount} |")
            
            # ç»„è£…æœ€ç»ˆè¡¨æ ¼
            table_result = '\n'.join(table_lines)
            
            print(f"  ğŸ“Š æˆåŠŸè§£æ {len(funding_list)} æ¡èèµ„ä¿¡æ¯")
            return f"\n{table_result}\n"
                
        except json.JSONDecodeError as e:
            print(f"  âŒ JSONè§£æå¤±è´¥: {e}")
            print(f"  ğŸ“ AIåŸå§‹å›å¤: {ai_result}")
            # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨åŸæœ‰çš„è¡¨æ ¼ä¿®å¤æ–¹æ³•
            return self._fix_table_format(ai_result)
        except Exception as e:
            print(f"  âŒ è¡¨æ ¼è§£æå¤±è´¥: {e}")
            return ""

    def _fix_table_format(self, table_content):
        """
        ä¿®å¤è¡¨æ ¼æ ¼å¼ï¼Œç¡®ä¿Markdownè¡¨æ ¼æ­£ç¡®æ˜¾ç¤º
        """
        if not table_content:
            return ""
        
        lines = table_content.split('\n')
        fixed_lines = []
        header_found = False
        separator_added = False
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # æ£€æŸ¥æ˜¯å¦æ˜¯è¡¨æ ¼è¡Œ
            if line.startswith('|') and line.endswith('|'):
                # ç¡®ä¿æ¯ä¸ªå•å…ƒæ ¼éƒ½æœ‰é€‚å½“çš„ç©ºæ ¼
                cells = line.split('|')
                formatted_cells = []
                
                for cell in cells:
                    cell = cell.strip()
                    if cell:  # éç©ºå•å…ƒæ ¼
                        formatted_cells.append(f" {cell} ")
                    else:  # ç©ºå•å…ƒæ ¼ï¼ˆå¼€å¤´æˆ–ç»“å°¾çš„|ï¼‰
                        formatted_cells.append("")
                
                # é‡æ–°ç»„è£…è¡Œ
                formatted_line = "|".join(formatted_cells)
                
                # å¦‚æœæ˜¯ç¬¬ä¸€è¡Œï¼ˆè¡¨å¤´ï¼‰ï¼Œæ·»åŠ åˆ†éš”ç¬¦
                if not header_found and any(keyword in formatted_line for keyword in ["å—èµ„æ–¹", "æŠ•èµ„æ–¹", "èµ›é“", "æ¶‰åŠé‡‘é¢"]):
                    fixed_lines.append(formatted_line)
                    # æ·»åŠ æ ‡å‡†çš„åˆ†éš”ç¬¦è¡Œ
                    separator = "| -------- | -------- | ------ | ---------- |"
                    fixed_lines.append(separator)
                    header_found = True
                    separator_added = True
                elif header_found and not separator_added:
                    # å¦‚æœè¡¨å¤´å·²æ‰¾åˆ°ä½†åˆ†éš”ç¬¦æœªæ·»åŠ 
                    separator = "| -------- | -------- | ------ | ---------- |"
                    fixed_lines.append(separator)
                    fixed_lines.append(formatted_line)
                    separator_added = True
                else:
                    fixed_lines.append(formatted_line)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¡¨å¤´ï¼Œæ·»åŠ æ ‡å‡†è¡¨å¤´
        if not header_found and fixed_lines:
            header = "| å—èµ„æ–¹ | æŠ•èµ„æ–¹ | èµ›é“ | æ¶‰åŠé‡‘é¢ |"
            separator = "| -------- | -------- | ------ | ---------- |"
            fixed_lines.insert(0, separator)
            fixed_lines.insert(0, header)
        
        result = '\n'.join(fixed_lines)
        
        # ç¡®ä¿è¡¨æ ¼å‰åæœ‰ç©ºè¡Œ
        if result:
            result = f"\n{result}\n"
        
        return result
    
    def cross_category_deduplication(self, filtered_results):
        """
        è·¨æ¿å—å»é‡ï¼šå¦‚æœæ–‡ç« åœ¨é«˜ä¼˜å…ˆçº§æ¿å—å‡ºç°ï¼Œåˆ™ä»ä½ä¼˜å…ˆçº§æ¿å—åˆ é™¤
        ä¼˜å…ˆçº§ï¼šPortfolio > é¡¹ç›®èèµ„ > åŸºé‡‘èèµ„ > å…¶ä»–æ¿å—
        """
        # å®šä¹‰ä¼˜å…ˆçº§é¡ºåº
        priority_categories = [
            ("portfolio", "Portfolio"),
            ("project", "é¡¹ç›®èèµ„"), 
            ("fund", "åŸºé‡‘èèµ„"),
            ("blockchain", "å…¬é“¾/L2/ä¸»ç½‘"),
            ("middleware", "ä¸­é—´ä»¶/å·¥å…·åè®®"),
            ("defi", "DeFi"),
            ("rwa", "RWA"),
            ("stablecoin", "ç¨³å®šå¸"),
            ("application", "åº”ç”¨åè®®"),
            ("gamefi", "GameFi"),
            ("exchange_wallet", "äº¤æ˜“æ‰€/é’±åŒ…"),
            ("ai_crypto", "AI + Crypto"),
            ("depin", "DePIN")
        ]
        
        # å­˜å‚¨å·²ç»åœ¨é«˜ä¼˜å…ˆçº§æ¿å—å‡ºç°çš„æ–‡ç« æ ‡é¢˜
        seen_titles = set()
        cross_dedup_stats = {}
        
        # æŒ‰ä¼˜å…ˆçº§å¤„ç†æ¯ä¸ªæ¿å—
        for category_key, category_name in priority_categories:
            articles = filtered_results.get(category_key, [])
            if not articles:
                cross_dedup_stats[category_name] = {'removed_count': 0}
                continue
                
            # å½“å‰æ¿å—çš„æ–‡ç« 
            current_titles = set()
            remaining_articles = []
            removed_count = 0
            
            for article in articles:
                article_title_clean = self._clean_title_for_comparison(article['title'])
                
                # æ£€æŸ¥æ˜¯å¦ä¸å·²å¤„ç†çš„é«˜ä¼˜å…ˆçº§æ¿å—æ–‡ç« é‡å¤
                is_duplicate = False
                for seen_title in seen_titles:
                    similarity = difflib.SequenceMatcher(None, article_title_clean, seen_title).ratio()
                    if similarity >= self.similarity_threshold:
                        is_duplicate = True
                        break
                
                if is_duplicate:
                    removed_count += 1
                    print(f"  âŒ ä»{category_name}åˆ é™¤é‡å¤æ–‡ç« : {article['title'][:50]}...")
                else:
                    remaining_articles.append(article)
                    current_titles.add(article_title_clean)
            
            # æ›´æ–°ç»“æœ
            filtered_results[category_key] = remaining_articles
            cross_dedup_stats[category_name] = {'removed_count': removed_count}
            
            # å°†å½“å‰æ¿å—çš„æ ‡é¢˜åŠ å…¥å·²è§æ ‡é¢˜é›†åˆ
            seen_titles.update(current_titles)
            
            if removed_count > 0:
                original_count = len(articles)
                remaining_count = len(remaining_articles)
                print(f"  ğŸ“Š {category_name}è·¨æ¿å—å»é‡: {original_count} â†’ {remaining_count} ç¯‡ (åˆ é™¤{removed_count}ç¯‡)")
        
        # ç»Ÿè®¡æ€»çš„è·¨æ¿å—å»é‡æƒ…å†µ
        total_cross_removed = sum(stats['removed_count'] for stats in cross_dedup_stats.values())
        if total_cross_removed > 0:
            print(f"ğŸ“Š è·¨æ¿å—å»é‡å®Œæˆï¼Œå…±åˆ é™¤ {total_cross_removed} ç¯‡é‡å¤æ–‡ç« ")
        else:
            print(f"ğŸ“Š è·¨æ¿å—å»é‡å®Œæˆï¼Œæœªå‘ç°é‡å¤æ–‡ç« ")
            
        return filtered_results, cross_dedup_stats

    def _get_legacy_prompt(self, category_name, count, articles_text):
        """æ—§çš„ç¡¬ç¼–ç promptæ–¹å¼ï¼ˆä½œä¸ºfallbackï¼‰"""
        # è¿™é‡Œå¯ä»¥ä¿ç•™åŸæ¥çš„prompté€»è¾‘ä½œä¸ºå¤‡ç”¨
        legacy_prompt = f"""è¯·åˆ†æä»¥ä¸‹{count}ç¯‡åŠ å¯†è´§å¸æ–°é—»ï¼Œåˆ¤æ–­å“ªäº›æ˜¯çœŸå®çš„{category_name}æ–°é—»ã€‚
        
è¯·åªè¿”å›ç¬¦åˆæ¡ä»¶çš„æ–‡ç« IDï¼Œæ ¼å¼ï¼š[id1, id2, id3]
å¦‚æœæ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æ–‡ç« ï¼Œè¿”å›ï¼š[]

æ–‡ç« åˆ—è¡¨ï¼š
{articles_text}"""
        return legacy_prompt

    def filter_batch_articles(self, articles_batch, article_type="project"):
        """æ‰¹å¤„ç†ç­›é€‰æ–‡ç« """
        # ä¸ºæ¯ç¯‡æ–‡ç« ç”Ÿæˆç®€çŸ­ID
        articles_text = ""
        id_mapping = {}
        
        for i, article in enumerate(articles_batch):
            article_id = f"ID{i+1}"
            title = article.get('title', '')
            content = article.get('content_text', '')[:100]
            id_mapping[article_id] = i
            if article_type in ["project", "blockchain", "middleware", "defi", "rwa", "stablecoin", "application", "gamefi", "exchange_wallet", "ai_crypto", "depin"]:
                articles_text += f"\n{article_id}: {title}\n"
            else:  # fund and others
                articles_text += f"\n{article_id}: {title}  - {content} \n"
        
        # æ˜ å°„æ–‡ç« ç±»å‹åˆ°ä¸­æ–‡ç±»åˆ«åç§°
        type_to_category = {
            "project": "é¡¹ç›®èèµ„",
            "fund": "åŸºé‡‘èèµ„",
            "blockchain": "å…¬é“¾/L2/ä¸»ç½‘",
            "middleware": "ä¸­é—´ä»¶/å·¥å…·åè®®",
            "defi": "DeFi",
            "rwa": "RWA",
            "stablecoin": "ç¨³å®šå¸",
            "application": "åº”ç”¨åè®®",
            "gamefi": "GameFi",
            "exchange_wallet": "äº¤æ˜“æ‰€/é’±åŒ…",
            "ai_crypto": "AI + Crypto",
            "depin": "DePIN"
        }
        
        category_name = type_to_category.get(article_type, "é¡¹ç›®èèµ„")
        
        # ä½¿ç”¨æ–°çš„promptç”Ÿæˆæ–¹æ³•
        prompt = self._generate_prompt(category_name, len(articles_batch), articles_text)
            
        
        try:
            # æ ¹æ®æä¾›å•†åˆ›å»ºå®¢æˆ·ç«¯
            if self.provider == "deepseek":
                client = openai.OpenAI(
                    api_key=self.api_key,
                    base_url=self.base_url
                )
            else:
                client = openai.OpenAI(api_key=self.api_key)
                
            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=200,
            )
            
            result = response.choices[0].message.content.strip()
            print(f"  ğŸ¤– AIè¿”å›: {result}")
            import sys
            sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¾“å‡º
            
            # è§£æè¿”å›çš„IDåˆ—è¡¨
            import re
            # Extract IDs from the AI response
            raw_ids = re.findall(r'\d+', result)
            # Add 'ID' prefix to each number
            id_matches = ['ID' + num for num in raw_ids]
            
            # è½¬æ¢ä¸ºæ–‡ç« ç´¢å¼•
            selected_indices = []
            for id_str in id_matches:
                if id_str in id_mapping:
                    selected_indices.append(id_mapping[id_str])
            
            return {
                'selected_indices': selected_indices,
                'ai_response': result,
                'success': True
            }
            
        except Exception as e:
            print(f"  âŒ APIè°ƒç”¨å¤±è´¥: {e}")
            return {
                'selected_indices': [],
                'ai_response': f"APIé”™è¯¯: {str(e)}",
                'success': False
            }
    
    def batch_filter_no_prompt(self, articles, batch_size=20, max_articles=None, article_type="project"):
        """æ‰¹é‡ç­›é€‰æ–‡ç«  - ä¸éœ€è¦ç”¨æˆ·ç¡®è®¤çš„ç‰ˆæœ¬"""
        if max_articles:
            articles = articles[:max_articles]
        
        type_name = self.TYPE_NAME_MAPPING.get(article_type, article_type)
        print(f"å¼€å§‹ä½¿ç”¨OpenAI APIæ‰¹é‡ç­›é€‰ {len(articles)} ç¯‡{type_name}æ–‡ç« ...")
        print(f"ğŸ“¦ æ‰¹å¤„ç†å¤§å°: {batch_size} ç¯‡/æ¬¡")
        
        # è®¡ç®—é¢„ä¼°æˆæœ¬
        num_batches = (len(articles) + batch_size - 1) // batch_size
        print(f"ğŸ“Š é¢„è®¡APIè°ƒç”¨æ¬¡æ•°: {num_batches} æ¬¡")
        
        filtered_articles = []
        
        # åˆ†æ‰¹å¤„ç†
        for batch_start in range(0, len(articles), batch_size):
            batch_end = min(batch_start + batch_size, len(articles))
            batch_articles = articles[batch_start:batch_end]
            batch_num = (batch_start // batch_size) + 1
            
            # è®¡ç®—å¹¶æ˜¾ç¤ºè¿›åº¦
            progress = int((batch_num / num_batches) * 100)
            print(f"\nğŸ“¦ å¤„ç†{type_name}æ‰¹æ¬¡ {batch_num}/{num_batches} ({len(batch_articles)} ç¯‡æ–‡ç« ) - è¿›åº¦: {progress}%")
            print(f"   æ–‡ç«  {batch_start+1}-{batch_end}")
            
            # è°ƒç”¨APIæ‰¹é‡ç­›é€‰ï¼Œä¼ å…¥æ–‡ç« ç±»å‹
            filter_result = self.filter_batch_articles(batch_articles, article_type)
            
            if filter_result['success']:
                selected_indices = filter_result['selected_indices']
                print(f"  âœ… é€‰ä¸­ {len(selected_indices)} ç¯‡: {selected_indices}")
                
                # æ·»åŠ é€‰ä¸­çš„æ–‡ç« 
                for idx in selected_indices:
                    if 0 <= idx < len(batch_articles):
                        filtered_articles.append(batch_articles[idx])
                        
            else:
                print(f"  âš ï¸ æ‰¹æ¬¡å¤„ç†å¤±è´¥ï¼Œä¿ç•™æ‰€æœ‰æ–‡ç« ")
                filtered_articles.extend(batch_articles)
            time.sleep(1)
        
        print(f"\nâœ… {type_name}ç­›é€‰å®Œæˆ: {len(articles)} â†’ {len(filtered_articles)} ç¯‡")
        return filtered_articles
    
    def batch_filter(self, articles, batch_size=20, max_articles=None, article_type="project"):
        """æ‰¹é‡ç­›é€‰æ–‡ç« """
        if max_articles:
            articles = articles[:max_articles]
        
        type_name = self.TYPE_NAME_MAPPING.get(article_type, article_type)
        print(f"å¼€å§‹ä½¿ç”¨OpenAI APIæ‰¹é‡ç­›é€‰ {len(articles)} ç¯‡{type_name}æ–‡ç« ...")
        print(f"ğŸ“¦ æ‰¹å¤„ç†å¤§å°: {batch_size} ç¯‡/æ¬¡")
        
        # è®¡ç®—é¢„ä¼°æˆæœ¬
        num_batches = (len(articles) + batch_size - 1) // batch_size
        print(f"ğŸ“Š é¢„è®¡APIè°ƒç”¨æ¬¡æ•°: {num_batches} æ¬¡")
        print("âš ï¸ æ³¨æ„ï¼šè¿™å°†æ¶ˆè€—APIè°ƒç”¨æ¬¡æ•°")
        
        # è¯¢é—®ç¡®è®¤
        confirm = input(f"ç¡®è®¤å¤„ç†{len(articles)}ç¯‡{type_name}æ–‡ç« ï¼Ÿ(y/n): ").strip().lower()
        if confirm != 'y':
            print("å–æ¶ˆå¤„ç†")
            return []
        
        filtered_articles = []
        
        # åˆ†æ‰¹å¤„ç†
        for batch_start in range(0, len(articles), batch_size):
            batch_end = min(batch_start + batch_size, len(articles))
            batch_articles = articles[batch_start:batch_end]
            batch_num = (batch_start // batch_size) + 1
            
            # è®¡ç®—å¹¶æ˜¾ç¤ºè¿›åº¦
            progress = int((batch_num / num_batches) * 100)
            print(f"\nğŸ“¦ å¤„ç†{type_name}æ‰¹æ¬¡ {batch_num}/{num_batches} ({len(batch_articles)} ç¯‡æ–‡ç« ) - è¿›åº¦: {progress}%")
            print(f"   æ–‡ç«  {batch_start+1}-{batch_end}")
            import sys
            sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¿›åº¦è¾“å‡º
            
            # æ˜¾ç¤ºå½“å‰æ‰¹æ¬¡æ–‡ç« æ ‡é¢˜
            for i, article in enumerate(batch_articles):
                print(f"   ID{i+1}: {article['title'][:60]}")
            sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°æ–‡ç« åˆ—è¡¨
            
            # è°ƒç”¨APIæ‰¹é‡ç­›é€‰ï¼Œä¼ å…¥æ–‡ç« ç±»å‹
            filter_result = self.filter_batch_articles(batch_articles, article_type)
            
            if filter_result['success']:
                selected_indices = filter_result['selected_indices']
                print(f"  âœ… é€‰ä¸­ {len(selected_indices)} ç¯‡: {selected_indices}")
                sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°ç»“æœè¾“å‡º
                
                # æ·»åŠ é€‰ä¸­çš„æ–‡ç« 
                for idx in selected_indices:
                    if 0 <= idx < len(batch_articles):
                        article_with_filter = batch_articles[idx].copy()
                        
                        
                        article_with_filter['ai_filter'] = {
                            'keep': True,
                            'reason': filter_result['ai_response'],
                            'batch_processed': True,
                            'batch_id': batch_num,
                            'article_type': article_type
                        }
                        
                        
                        filtered_articles.append(article_with_filter)
            else:
                print(f"  âŒ æ‰¹æ¬¡å¤„ç†å¤±è´¥")
            
            # é¿å…APIé€Ÿç‡é™åˆ¶
            if batch_num < num_batches:
                print(f"  â³ æš‚åœ1ç§’...")
                time.sleep(1)
        
        print(f"\nğŸ¯ {type_name}æ‰¹é‡ç­›é€‰å®Œæˆï¼")
        print(f"åŸå§‹æ–‡ç« : {len(articles)} ç¯‡")
        print(f"ç­›é€‰å: {len(filtered_articles)} ç¯‡")
        print(f"è¿‡æ»¤ç‡: {(1 - len(filtered_articles)/len(articles))*100:.1f}%")
        print(f"APIè°ƒç”¨æ¬¡æ•°: {num_batches} æ¬¡")
        
        return filtered_articles

def main():
    # è¯»å–æœ€æ–°çš„åˆ†ç±»ç»“æœæ–‡ä»¶
    latest_file = "latest_classified.json"
    
    # å¦‚æœæœ€æ–°æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•æŸ¥æ‰¾æ—§æ ¼å¼çš„æ–‡ä»¶
    if not os.path.exists(latest_file):
        print("âŒ æœªæ‰¾åˆ°åˆ†ç±»ç»“æœæ–‡ä»¶")
        print("è¯·å…ˆè¿è¡Œ python classify_articles.py")
        return
    
    print(f"è¯»å–æ–‡ä»¶: {latest_file}")
    
    try:
        with open(latest_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return
    
    # è·å–å„ç±»æ–‡ç« 
    articles_by_category = data.get('articles_by_category', {})
    project_funding_articles = articles_by_category.get('é¡¹ç›®èèµ„', [])
    fund_funding_articles = articles_by_category.get('åŸºé‡‘èèµ„', [])
    blockchain_articles = articles_by_category.get('å…¬é“¾/L2/ä¸»ç½‘', [])
    middleware_articles = articles_by_category.get('ä¸­é—´ä»¶/å·¥å…·åè®®', [])
    defi_articles = articles_by_category.get('DeFi', [])
    rwa_articles = articles_by_category.get('RWA', [])
    stablecoin_articles = articles_by_category.get('ç¨³å®šå¸', [])
    application_articles = articles_by_category.get('åº”ç”¨åè®®', [])
    gamefi_articles = articles_by_category.get('GameFi', [])
    exchange_wallet_articles = articles_by_category.get('äº¤æ˜“æ‰€/é’±åŒ…', [])
    ai_crypto_articles = articles_by_category.get('AI + Crypto', [])
    depin_articles = articles_by_category.get('DePIN', [])
    portfolio_articles = articles_by_category.get('portfolios', [])
    
    # è®¡ç®—æ€»æ–‡ç« æ•°
    total_articles = (len(project_funding_articles) + len(fund_funding_articles) + 
                     len(blockchain_articles) + len(middleware_articles) + 
                     len(defi_articles) + len(rwa_articles) + len(stablecoin_articles) +
                     len(application_articles) + len(gamefi_articles) + 
                     len(exchange_wallet_articles) + len(ai_crypto_articles) + 
                     len(depin_articles) + len(portfolio_articles))
    
    if total_articles == 0:
        print("âŒ æœªæ‰¾åˆ°å¯ç­›é€‰çš„æ–‡ç« ")
        return
    
    print(f"=== AI æ–‡ç« ç­›é€‰å™¨ ===")
    print(f"æ‰¾åˆ°é¡¹ç›®èèµ„åˆ†ç±»æ–‡ç« : {len(project_funding_articles)} ç¯‡")
    print(f"æ‰¾åˆ°åŸºé‡‘èèµ„åˆ†ç±»æ–‡ç« : {len(fund_funding_articles)} ç¯‡")
    print(f"æ‰¾åˆ°å…¬é“¾/L2/ä¸»ç½‘æ–‡ç« : {len(blockchain_articles)} ç¯‡")
    print(f"æ‰¾åˆ°ä¸­é—´ä»¶/å·¥å…·åè®®æ–‡ç« : {len(middleware_articles)} ç¯‡")
    print(f"æ‰¾åˆ°DeFiæ–‡ç« : {len(defi_articles)} ç¯‡")
    print(f"æ‰¾åˆ°RWAæ–‡ç« : {len(rwa_articles)} ç¯‡")
    print(f"æ‰¾åˆ°ç¨³å®šå¸æ–‡ç« : {len(stablecoin_articles)} ç¯‡")
    print(f"æ‰¾åˆ°åº”ç”¨åè®®æ–‡ç« : {len(application_articles)} ç¯‡")
    print(f"æ‰¾åˆ°GameFiæ–‡ç« : {len(gamefi_articles)} ç¯‡")
    print(f"æ‰¾åˆ°äº¤æ˜“æ‰€/é’±åŒ…æ–‡ç« : {len(exchange_wallet_articles)} ç¯‡")
    print(f"æ‰¾åˆ°AI + Cryptoæ–‡ç« : {len(ai_crypto_articles)} ç¯‡")
    print(f"æ‰¾åˆ°DePINæ–‡ç« : {len(depin_articles)} ç¯‡")
    print(f"æ‰¾åˆ°Portfolioæ–‡ç« : {len(portfolio_articles)} ç¯‡")
    
    # é€‰æ‹©APIæä¾›å•†
    print(f"\n=== é€‰æ‹©AI APIæä¾›å•† ===")
    print("1. OpenAI API (åå°é…ç½®)")
    print("2. OpenAI API (ç”¨æˆ·è¾“å…¥)")
    
    provider_choice = input("è¯·é€‰æ‹©APIæä¾›å•† (1/2ï¼Œé»˜è®¤1): ").strip()
    
    api_key = None
    provider = "openai"
    
    if provider_choice == "2":
        provider = "openai"
        api_key = input("è¯·è¾“å…¥OpenAI APIå¯†é’¥: ").strip()
        if not api_key:
            print("âŒ æœªæä¾›OpenAI APIå¯†é’¥")
            return
    else:
        print("âœ… ä½¿ç”¨åå°API")
    
    # åˆ›å»ºç­›é€‰å™¨
    filter = AIFundingFilter(api_key, provider)
    if not filter.api_key:
        print("âŒ APIå¯†é’¥é…ç½®å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
        return
    
    # è¯¢é—®å¤„ç†æ•°é‡å’Œæ‰¹å¤§å°
    max_articles = input(f"è¦å¤„ç†å¤šå°‘ç¯‡æ–‡ç« ï¼Ÿ(é»˜è®¤å…¨éƒ¨{total_articles}ç¯‡ï¼Œè¾“å…¥æ•°å­—é™åˆ¶æ•°é‡): ").strip()
    if max_articles.isdigit():
        max_articles = int(max_articles)
    else:
        max_articles = None
    
    batch_size = input("æ‰¹å¤„ç†å¤§å°ï¼Ÿ(é»˜è®¤20ç¯‡/æ¬¡ï¼Œè¾“å…¥æ•°å­—ä¿®æ”¹): ").strip()
    if batch_size.isdigit():
        batch_size = int(batch_size)
    else:
        batch_size = 20
    
    # è¯¢é—®æ˜¯å¦ä½¿ç”¨AIæ‰¹é‡è¯­ä¹‰å»é‡
    print(f"\n=== å»é‡æ–¹å¼é€‰æ‹© ===")
    print("æ˜¯å¦åœ¨å­—ç¬¦ç›¸ä¼¼åº¦å»é‡åä½¿ç”¨AIæ‰¹é‡è¯­ä¹‰å»é‡ï¼Ÿ")
    print("AIè¯­ä¹‰å»é‡èƒ½è¯†åˆ«æ„æ€ç›¸åŒä½†è¡¨è¿°ä¸åŒçš„é‡å¤æ–‡ç« ï¼Œä½†ä¼šæ¶ˆè€—é¢å¤–APIè°ƒç”¨")
    
    ai_dedup_choice = input("ä½¿ç”¨AIæ‰¹é‡è¯­ä¹‰å»é‡ï¼Ÿ(y/nï¼Œé»˜è®¤y): ").strip().lower()
    use_ai_semantic_dedup = ai_dedup_choice != "n"
    
    if use_ai_semantic_dedup:
        print("âœ… å°†ä½¿ç”¨AIæ‰¹é‡è¯­ä¹‰å»é‡")
    else:
        print("âœ… ä»…ä½¿ç”¨å­—ç¬¦ç›¸ä¼¼åº¦å»é‡")
    
    # å®šä¹‰æ‰€æœ‰ç±»åˆ«çš„é…ç½®
    categories_config = [
        ("é¡¹ç›®èèµ„", project_funding_articles, "project"),
        ("åŸºé‡‘èèµ„", fund_funding_articles, "fund"),
        ("å…¬é“¾/L2/ä¸»ç½‘", blockchain_articles, "blockchain"),
        ("ä¸­é—´ä»¶/å·¥å…·åè®®", middleware_articles, "middleware"),
        ("DeFi", defi_articles, "defi"),
        ("RWA", rwa_articles, "rwa"),
        ("ç¨³å®šå¸", stablecoin_articles, "stablecoin"),
        ("åº”ç”¨åè®®", application_articles, "application"),
        ("GameFi", gamefi_articles, "gamefi"),
        ("äº¤æ˜“æ‰€/é’±åŒ…", exchange_wallet_articles, "exchange_wallet"),
        ("AI + Crypto", ai_crypto_articles, "ai_crypto"),
        ("DePIN", depin_articles, "depin")
    ]
    
    # ç»Ÿä¸€ç­›é€‰å„ç±»æ–‡ç« 
    filtered_results = {}
    dedup_stats = {}
    
    for category_name, articles, article_type in categories_config:
        if articles:
            print(f"\nğŸ¯ å¼€å§‹ç­›é€‰{category_name}æ–‡ç« ...")
            # å…ˆè¿›è¡ŒAIç­›é€‰
            ai_filtered = filter.batch_filter(articles, batch_size, max_articles, article_type)
            
            # å†è¿›è¡Œæ ‡é¢˜å»é‡
            if ai_filtered:
                print(f"\nğŸ”„ å¯¹{category_name}è¿›è¡Œå­—ç¬¦ç›¸ä¼¼åº¦å»é‡...")
                deduplicated, dedup_stat = filter.deduplicate_articles_by_title(ai_filtered, category_name)
                filtered_results[article_type] = deduplicated
                dedup_stats[category_name] = dedup_stat
            else:
                filtered_results[article_type] = []
                dedup_stats[category_name] = {'removed_count': 0, 'duplicate_groups': [], 'removal_rate': 0}
        else:
            filtered_results[article_type] = []
            dedup_stats[category_name] = {'removed_count': 0, 'duplicate_groups': [], 'removal_rate': 0}
    
    # Portfolioæ–‡ç« ä¸éœ€è¦AIç­›é€‰ï¼Œä½†éœ€è¦å»é‡
    if portfolio_articles:
        print(f"\nâ­ Portfolioæ–‡ç« æ— éœ€AIç­›é€‰ï¼Œç›´æ¥è¿›è¡Œå­—ç¬¦ç›¸ä¼¼åº¦å»é‡...")
        deduplicated_portfolio, portfolio_dedup_stat = filter.deduplicate_articles_by_title(portfolio_articles, "Portfolio")
        filtered_results["portfolio"] = deduplicated_portfolio
        dedup_stats["Portfolio"] = portfolio_dedup_stat
    else:
        filtered_results["portfolio"] = []
        dedup_stats["Portfolio"] = {'removed_count': 0, 'duplicate_groups': [], 'removal_rate': 0}
    
    # è·¨æ¿å—å»é‡ï¼šä¼˜å…ˆçº§ Portfolio > é¡¹ç›®èèµ„ > åŸºé‡‘èèµ„ > å…¶ä»–æ¿å—
    print(f"\nğŸ”„ æ‰§è¡Œè·¨æ¿å—å»é‡...")
    filtered_results, cross_dedup_stats = filter.cross_category_deduplication(filtered_results)
    
    # AIæ‰¹é‡è¯­ä¹‰å»é‡ï¼ˆå¯é€‰ï¼‰
    ai_semantic_dedup_stats = {'removed_count': 0, 'duplicate_groups': []}
    if use_ai_semantic_dedup:
        filtered_results, ai_semantic_dedup_stats = filter.ai_batch_semantic_deduplication(filtered_results)
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
 
    # ç”Ÿæˆæ ¼å¼åŒ–è¾“å‡º
    suffix = "_ai_deduplicated" if use_ai_semantic_dedup else "_deduplicated"
    formatted_output_file = f"formatted_report_{timestamp}{suffix}.txt"
    with open(formatted_output_file, 'w', encoding='utf-8') as f:
        def clean_content(content):
            import re
            # åˆ é™¤å„ç§æ–°é—»æ¥æºå‰ç¼€ï¼ŒåŒ…æ‹¬å¸¦ç©ºæ ¼çš„æ¨¡å¼
            content = re.sub(r'.*?æ¶ˆæ¯[ï¼Œ,]\s*', '', content)
            content = re.sub(r'.*?æŠ¥é“[ï¼Œ,]\s*', '', content)
            content = re.sub(r'æ·±æ½®\s*TechFlow\s*[æ¶ˆæ¯æŠ¥é“]*[ï¼Œ,]\s*', '', content)
            content = re.sub(r'TechFlow\s*æ·±æ½®\s*[æ¶ˆæ¯æŠ¥é“]*[ï¼Œ,]\s*', '', content)
            content = re.sub(r'PANews\s*\d+æœˆ\d+æ—¥æ¶ˆæ¯[ï¼Œ,]\s*', '', content)
            content = re.sub(r'Wu\s*Blockchain\s*[æ¶ˆæ¯æŠ¥é“]*[ï¼Œ,]\s*', '', content)
            content = re.sub(r'Cointelegraph\s*ä¸­æ–‡\s*[æ¶ˆæ¯æŠ¥é“]*[ï¼Œ,]\s*', '', content)
            # åˆ é™¤å¤šä½™çš„ç©ºç™½
            content = re.sub(r'\s+', ' ', content).strip()
            return content
        
        def delete_reports(articles):
            new_articles = []
            for article in articles:
                if not re.search(r"å‘¨æŠ¥|æ—¥æŠ¥|æœˆæŠ¥", article['title']) or len(article.get('content_text', '')) > 200:
                    new_articles.append(article)
            return new_articles

        # æ”¶é›†æ‰€æœ‰æ–‡ç« å¹¶æŒ‰å†…å®¹é•¿åº¦åˆ†ç±»
        all_sections = [
            ("é¡¹ç›®èèµ„", filtered_results.get("project", []), "é¡¹ç›®èèµ„ä»‹ç»"),
            ("åŸºé‡‘èèµ„", filtered_results.get("fund", []), "åŸºé‡‘èèµ„ä»‹ç»"),
            ("å…¬é“¾/L2/ä¸»ç½‘", filtered_results.get("blockchain", []), "å…¬é“¾/L2/ä¸»ç½‘"),
            ("ä¸­é—´ä»¶/å·¥å…·åè®®", filtered_results.get("middleware", []), "ä¸­é—´ä»¶/å·¥å…·åè®®"),
            ("DeFi", filtered_results.get("defi", []), "DeFi"),
            ("RWA", filtered_results.get("rwa", []), "RWA"),
            ("ç¨³å®šå¸", filtered_results.get("stablecoin", []), "ç¨³å®šå¸"),
            ("åº”ç”¨åè®®", filtered_results.get("application", []), "åº”ç”¨åè®®"),
            ("GameFi", filtered_results.get("gamefi", []), "GameFi"),
            ("äº¤æ˜“æ‰€/é’±åŒ…", filtered_results.get("exchange_wallet", []), "äº¤æ˜“æ‰€/é’±åŒ…"),
            ("AI + Crypto", filtered_results.get("ai_crypto", []), "AI + Crypto"),
            ("DePIN", filtered_results.get("depin", []), "DePIN"),
            ("Portfolio", filtered_results.get("portfolio", []), "Our portfolio")
        ]
        
        # æŒ‰å†…å®¹é•¿åº¦åˆ†ç±»æ–‡ç« 
        news_articles = {}  
        report_articles = {}  
        
        for category_name, articles_list, section_title in all_sections:
            if articles_list:
                news_articles[category_name] = {'title': section_title, 'articles': []}
                report_articles[category_name] = {'title': section_title, 'articles': []}
                
                for article in delete_reports(articles_list):
                    content = clean_content(article.get('content_text', ''))
                    
                    # æ ¹æ®å†…å®¹é•¿åº¦åˆ†ç±»
                    if len(content) > 700:
                        report_articles[category_name]['articles'].append({
                            'title': article['title'],
                            'url': article['url'],
                            'content': content
                        })
                    else:
                        news_articles[category_name]['articles'].append({
                            'title': article['title'],
                            'url': article['url'],
                            'content': content
                        })
        
        # ä½¿ç”¨å…¨å±€ç¼–å·
        global_counter = 1
        
        # å…ˆå†™å…¥æ–°é—»æ¿å—
        f.write("# ğŸ“° æ–°é—»\n\n")
        
        # ä¸ºèèµ„ç›¸å…³æ¿å—ç”Ÿæˆè¡¨æ ¼
        funding_table = ""
        project_funding_articles = []
        fund_funding_articles = []
        
        # æ”¶é›†é¡¹ç›®èèµ„å’ŒåŸºé‡‘èèµ„çš„æ–°é—»æ–‡ç« 
        if "é¡¹ç›®èèµ„" in news_articles and news_articles["é¡¹ç›®èèµ„"]['articles']:
            project_funding_articles = news_articles["é¡¹ç›®èèµ„"]['articles']
            
        # ç”Ÿæˆèèµ„ä¿¡æ¯è¡¨æ ¼
        if project_funding_articles:
            funding_table = filter._generate_funding_table_with_ai(project_funding_articles)

        for category_name, category_data in news_articles.items():
            if category_data['articles']:
                f.write(f"## {category_data['title']}\n\n")
                
                # å¦‚æœæ˜¯èèµ„ç›¸å…³æ¿å—ä¸”æœ‰è¡¨æ ¼ï¼Œå…ˆå†™å…¥è¡¨æ ¼
                if "é¡¹ç›®èèµ„" in category_name and funding_table:
                    print(f"  ğŸ“Š æ­£åœ¨å†™å…¥æŠ•èèµ„æ¸…å•è¡¨æ ¼åˆ°æŠ¥å‘Š...")
                    f.write("### ğŸ“Š æŠ•èèµ„æ¸…å•\n\n")
                    f.write(funding_table)
                    f.write("\n\n### ğŸ“° è¯¦ç»†æ–°é—»\n\n")
                
                for article in category_data['articles']:
                    f.write(f"{global_counter}. [{article['title']}]({article['url']})\n\n{article['content']}\n\n")
                    global_counter += 1
        
        # å†å†™å…¥ç ”æŠ¥æ¿å—ï¼ˆä½¿ç”¨AIæ€»ç»“ï¼‰
        f.write("# ğŸ“Š ç ”æŠ¥\n\n")
        
        # ä¸ºèèµ„ç ”æŠ¥ç”Ÿæˆè¡¨æ ¼
        report_funding_table = ""
        
        for category_name, category_data in report_articles.items():
            if category_data['articles']:
                f.write(f"## {category_data['title']}\n\n")
                
                for article in category_data['articles']:
                    # ä½¿ç”¨AIæ€»ç»“ç ”æŠ¥å†…å®¹
                    print(f"  ğŸ¤– æ­£åœ¨AIæ€»ç»“ç ”æŠ¥: {article['title'][:50]}...")
                    summarized_content = filter._summarize_report_with_ai(article['content'])
                    f.write(f"{global_counter}. [{article['title']}]({article['url']})\n\n{summarized_content}\n\n")
                    global_counter += 1
    
    print(f"ğŸ“‹ æ ¼å¼åŒ–è¾“å‡ºå·²ä¿å­˜åˆ°: {formatted_output_file}")
    
    # ç»Ÿè®¡æ–°é—»å’Œç ”æŠ¥æ•°é‡
    total_news = sum(len(data['articles']) for data in news_articles.values())
    total_reports = sum(len(data['articles']) for data in report_articles.values())
    
    print(f"\nğŸ“Š æ–‡ç« åˆ†ç±»ç»Ÿè®¡:")
    print(f"   ğŸ“° æ–°é—»: {total_news} ç¯‡")
    print(f"   ğŸ“Š ç ”æŠ¥: {total_reports} ç¯‡ (å·²AIæ€»ç»“)")
    print(f"   æ€»è®¡: {total_news + total_reports} ç¯‡")
    
    print(f"\nğŸ¯ AIç­›é€‰ + æ ‡é¢˜å»é‡ + è·¨æ¿å—å»é‡ + æ–°é—»ç ”æŠ¥åˆ†ç±» + AIç ”æŠ¥æ€»ç»“å®Œæˆï¼")
    
    # ç»Ÿè®¡è¾“å‡ºï¼Œä½¿ç”¨å¾ªç¯
    stats_config = [
        ("é¡¹ç›®èèµ„", project_funding_articles, "project"),
        ("åŸºé‡‘èèµ„", fund_funding_articles, "fund"),
        ("å…¬é“¾/L2/ä¸»ç½‘", blockchain_articles, "blockchain"),
        ("ä¸­é—´ä»¶/å·¥å…·åè®®", middleware_articles, "middleware"),
        ("DeFi", defi_articles, "defi"),
        ("RWA", rwa_articles, "rwa"),
        ("ç¨³å®šå¸", stablecoin_articles, "stablecoin"),
        ("åº”ç”¨åè®®", application_articles, "application"),
        ("GameFi", gamefi_articles, "gamefi"),
        ("äº¤æ˜“æ‰€/é’±åŒ…", exchange_wallet_articles, "exchange_wallet"),
        ("AI + Crypto", ai_crypto_articles, "ai_crypto"),
        ("DePIN", depin_articles, "depin"),
        ("Portfolio", portfolio_articles, "portfolio")
    ]
    
    for category_name, original_articles, result_key in stats_config:
        filtered_count = len(filtered_results.get(result_key, []))
        dedup_removed = dedup_stats.get(category_name, {}).get('removed_count', 0)
        cross_removed = cross_dedup_stats.get(category_name, {}).get('removed_count', 0)
        
        removal_info = []
        if dedup_removed > 0:
            removal_info.append(f"æ ‡é¢˜å»é‡{dedup_removed}ç¯‡")
        if cross_removed > 0:
            removal_info.append(f"è·¨æ¿å—å»é‡{cross_removed}ç¯‡")
            
        if removal_info:
            removal_str = f" ({', '.join(removal_info)})"
            print(f"   {category_name}: {len(original_articles)} â†’ {filtered_count} ç¯‡{removal_str}")
        else:
            print(f"   {category_name}: {len(original_articles)} â†’ {filtered_count} ç¯‡")
    
    # è®¡ç®—æ€»è®¡
    total_original = sum(len(articles) for _, articles, _ in stats_config)
    total_filtered = sum(len(filtered_results.get(result_key, [])) for _, _, result_key in stats_config)
    total_dedup_removed = sum(dedup_stats.get(cat_name, {}).get('removed_count', 0) for cat_name, _, _ in stats_config)
    total_cross_removed = sum(cross_dedup_stats.get(cat_name, {}).get('removed_count', 0) for cat_name, _, _ in stats_config)
    
    print(f"   æ€»è®¡: {total_original} â†’ {total_filtered} ç¯‡")
    
    # è®¡ç®—AIè¯­ä¹‰å»é‡åˆ é™¤æ•°é‡
    ai_semantic_removed = ai_semantic_dedup_stats.get('removed_count', 0)
    
    if total_dedup_removed > 0 or total_cross_removed > 0 or ai_semantic_removed > 0:
        print(f"\nğŸ“Š å»é‡ç»Ÿè®¡æ±‡æ€»:")
        if total_dedup_removed > 0:
            print(f"   å­—ç¬¦ç›¸ä¼¼åº¦å»é‡åˆ é™¤: {total_dedup_removed} ç¯‡ ({(total_dedup_removed / total_original * 100):.1f}%)")
        if total_cross_removed > 0:
            print(f"   è·¨æ¿å—å»é‡åˆ é™¤: {total_cross_removed} ç¯‡ ({(total_cross_removed / total_original * 100):.1f}%)")
        if ai_semantic_removed > 0:
            print(f"   AIè¯­ä¹‰å»é‡åˆ é™¤: {ai_semantic_removed} ç¯‡ ({(ai_semantic_removed / total_original * 100):.1f}%)")
        
        total_removed = total_dedup_removed + total_cross_removed + ai_semantic_removed
        print(f"   æ€»å»é‡åˆ é™¤: {total_removed} ç¯‡ ({(total_removed / total_original * 100):.1f}%)")
        
        # æ˜¾ç¤ºAIè¯­ä¹‰å»é‡ç»„æ•°
        if ai_semantic_removed > 0:
            duplicate_groups_count = len(ai_semantic_dedup_stats.get('duplicate_groups', []))
            print(f"   AIè¯†åˆ«é‡å¤ç»„: {duplicate_groups_count} ç»„")
    
    
    # åœ¨æ§åˆ¶å°æ˜¾ç¤ºæ ¼å¼åŒ–è¾“å‡ºé¢„è§ˆ
    print(f"\n=== æ ¼å¼åŒ–è¾“å‡ºé¢„è§ˆ ===")
    
    def clean_content_preview(content):
        import re
        content = re.sub(r'.*?æ¶ˆæ¯[ï¼Œ,]\s*', '', content)
        content = re.sub(r'.*?æŠ¥é“[ï¼Œ,]\s*', '', content)
        content = re.sub(r'æ·±æ½®\s*TechFlow\s*[æ¶ˆæ¯æŠ¥é“]*[ï¼Œ,]\s*', '', content)
        content = re.sub(r'TechFlow\s*æ·±æ½®\s*[æ¶ˆæ¯æŠ¥é“]*[ï¼Œ,]\s*', '', content)
        content = re.sub(r'PANews\s*\d+æœˆ\d+æ—¥æ¶ˆæ¯[ï¼Œ,]\s*', '', content)
        content = re.sub(r'Wu\s*Blockchain\s*[æ¶ˆæ¯æŠ¥é“]*[ï¼Œ,]\s*', '', content)
        content = re.sub(r'Cointelegraph\s*ä¸­æ–‡\s*[æ¶ˆæ¯æŠ¥é“]*[ï¼Œ,]\s*', '', content)
        content = re.sub(r'\s+', ' ', content).strip()
        return content
    
    # é¢„è§ˆæ–°é—»éƒ¨åˆ†ï¼ˆé¡¹ç›®èèµ„å’ŒåŸºé‡‘èèµ„çš„å‰å‡ ç¯‡çŸ­æ–°é—»ï¼‰
    print(f"\n# ğŸ“° æ–°é—»é¢„è§ˆ")
    preview_counter = 1
    for category in ['é¡¹ç›®èèµ„', 'åŸºé‡‘èèµ„']:
        if category in news_articles and news_articles[category]['articles']:
            print(f"\n## {news_articles[category]['title']}")
            for article in news_articles[category]['articles'][:2]:
                preview_content = article['content'][:100] + "..." if len(article['content']) > 100 else article['content']
                print(f"\n{preview_counter}. [{article['title']}]({article['url']})")
                print(f"{preview_content}")
                preview_counter += 1
    
    # é¢„è§ˆç ”æŠ¥éƒ¨åˆ†ï¼ˆæ˜¾ç¤ºAIæ€»ç»“æ•ˆæœï¼‰
    print(f"\n# ğŸ“Š ç ”æŠ¥é¢„è§ˆ (AIæ€»ç»“)")
    for category in ['é¡¹ç›®èèµ„', 'DeFi']:
        if category in report_articles and report_articles[category]['articles']:
            print(f"\n## {report_articles[category]['title']}")
            for article in report_articles[category]['articles'][:1]:
                print(f"\n{preview_counter}. [{article['title']}]({article['url']})")
                print(f"[åŸæ–‡é•¿åº¦: {len(article['content'])} å­—ç¬¦ï¼Œå°†ä½¿ç”¨AIè¿›è¡Œç»“æ„åŒ–æ€»ç»“]")
                
                # æ˜¾ç¤ºAIæ€»ç»“çš„æ ¼å¼ç¤ºä¾‹
                print("é¢„æœŸAIæ€»ç»“æ ¼å¼:")
                print("=== æ®µè½ç²¾ç‚¼ ===")
                print("[1] æ®µè½æ‘˜è¦")
                print("[2] æ®µè½æ‘˜è¦")
                print("...")
                print("\n=== æ°´ä¸‹ä¿¡æ¯æå– ===")
                print("- æ°´ä¸‹ä¿¡æ¯ 1")
                print("- æ°´ä¸‹ä¿¡æ¯ 2")
                print("...")
                print("\n=== é‡‘å¥æç‚¼ ===")
                print("â˜… é‡‘å¥ 1")
                print("â˜… é‡‘å¥ 2")
                print("...")
                
                preview_counter += 1
                break

if __name__ == "__main__":
    main()