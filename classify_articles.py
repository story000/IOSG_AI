#!/usr/bin/env python3
"""
åŠ å¯†è´§å¸æ–‡ç« åˆ†ç±»å™¨
æ ¹æ®å…³é”®è¯å¯¹æ–‡ç« è¿›è¡Œåˆ†ç±»
"""

import json
import re
from datetime import datetime
from collections import defaultdict

class CryptoArticleClassifier:
    def __init__(self):
        # å®šä¹‰åˆ†ç±»å’Œå¯¹åº”çš„å…³é”®è¯å’Œæ¨¡å¼
        self.portfolios = [
            "Autonomys", "Avalanche", "Celestia", "Conflux", "Cosmos", "Dfinity", "IoTex", 
            "Marlin", "Mina", "Near", "Oasis Labs", "Phala", "Polkadot", "Monad", "0x", 
            "Morpho", "Brink", "Centrifuge", "ChainSafe", "DeBank", "dHEDGE", "DODO", 
            "Bluefin", "Impossible Finance", "Kyber", "MakerDAO", "Mangata", "MCDEX", 
            "Metapool", "Orderly Network", "prePO", "Solv", "SynFutures", "Synthetix", 
            "Transak", "UMA", "Volmex Finance", "Wootrade", "Arbitrum", "Aurora", "Aztec", 
            "BOB", "Celer", "Connext", "Debridge", "Fhenix", "Moonbeam", "NIL", "Scroll", 
            "Starkware", "Taiko", "zkSync", "Optimism", "Polygon", "Atherscope", "AltLayer", 
            "Arweave", "Automata", "Babylon", "Blocknative", "CARV", "ConsenSys", "Covalent", 
            "Dappback", "EigenLayer", "Filecoin", "Flashbots", "Gelato", "Infura", "Ingonyama", 
            "Kiln", "Kyve", "Liquifi", "Lisk", "Lurk Labs", "Plasm", "Astar Network", 
            "Primev", "Renzo", "Redstone", "REDPILL", "Space and Time", "zCloak network", 
            "Swell", "3rm", "WeaveDB", "Ancient8", "Artifact", "Mixmob", "Big Time", 
            "Blade DAO", "NOR", "Gomble Games", "Illuvium", "Playmint", "Polemos", 
            "Shrapnel", "The Beacon", "Kettle", "Alethea AI", "CyberConnect", "ETHSign", 
            "GALXE", "Mintbase", "Mintgate", "PIANITY", "RMRK", "Roll", "Ardrive", "Coin98", 
            "Kravata", "Push", "Safe", "Mask Network", "MetaMask", "Onekey", "DAOhaus", 
            "DAOSquare", "DeFi Alliance", "Gitcoin", "LearnWeb3", "MetaCartel", 
            "Permanent Ventures", "Seed Club", "Arkhivist", "Audit Wizard", "Hats", 
            "Runtime Verification"
        ]

        self.categories = {
            "é¡¹ç›®èèµ„": {
                "keywords": [
                    "Aè½®", "Bè½®", "Cè½®", "ç§å­è½®", "å¤©ä½¿è½®", "pre-seed", "seed",
                    "Series A", "Series B", "Series C", "é¢†æŠ•", "è·ŸæŠ•", "ä¼°å€¼", "é£æŠ•", "VC",
                    "èå¾—", "ç­¹é›†", "ç­¹åˆ°", "å‹Ÿèµ„", "å‹Ÿå¾—", "è½®èèµ„", "æŠ•èµ„æ–¹",
                    "æŠ•èµ„æœºæ„", "æˆ˜ç•¥æŠ•èµ„", "pre-A", "A+è½®", "B+è½®", "oversubscribed",
                    "funding", "investment", "investor", "venture", "capital", "round"
                ],
                "patterns": [
                    r"å®Œæˆ.*?èèµ„",
                    r"è·å¾—.*?èèµ„", 
                    r"å®£å¸ƒ.*?èèµ„",
                    r"èèµ„.*?ä¸‡ç¾å…ƒ",
                    r"èèµ„.*?åƒä¸‡ç¾å…ƒ", 
                    r"èèµ„.*?äº¿ç¾å…ƒ",
                    r"è·æŠ•.*?ä¸‡ç¾å…ƒ",
                    r"è·æŠ•.*?åƒä¸‡ç¾å…ƒ",
                    r"è·æŠ•.*?äº¿ç¾å…ƒ",
                    r"ç­¹é›†.*?ä¸‡ç¾å…ƒ",
                    r"ç­¹é›†.*?åƒä¸‡ç¾å…ƒ",
                    r"ç­¹é›†.*?äº¿ç¾å…ƒ",
                    r"raised.*?\$.*?million",
                    r"raised.*?\$.*?billion",
                    r"funding.*?\$.*?million",
                    r"funding.*?\$.*?billion",
                    r"æŠ•èµ„.*?ä¸‡ç¾å…ƒ",
                    r"æŠ•èµ„.*?åƒä¸‡ç¾å…ƒ",
                    r"æŠ•èµ„.*?äº¿ç¾å…ƒ"
                ],
                "weight": 1.0
            },
            "åŸºé‡‘èèµ„": {
                "keywords": [
                    "åŸºé‡‘", "fund", "èµ„ç®¡", "LP", "ç®¡ç†è§„æ¨¡", "AUM", "å‹Ÿé›†åŸºé‡‘", "åŸºé‡‘ç®¡ç†",
                    "æŠ•èµ„åŸºé‡‘", "crypto fund", "åŒºå—é“¾åŸºé‡‘", "æ•°å­—èµ„äº§åŸºé‡‘", "é£é™©æŠ•èµ„åŸºé‡‘",
                    "hedge fund", "å¯¹å†²åŸºé‡‘", "ç§å‹ŸåŸºé‡‘", "å…¬å‹ŸåŸºé‡‘", "åŸºé‡‘è§„æ¨¡", "åŸºé‡‘æˆç«‹",
                    "åŸºé‡‘å¯åŠ¨", "åŸºé‡‘å‹Ÿèµ„", "èµ„äº§ç®¡ç†", "investment fund", "venture fund",
                    "æ–°åŸºé‡‘", "åŸºé‡‘å…¬å¸", "èµ„ç®¡å…¬å¸", "åŸºé‡‘åˆä¼™äºº", "GP", "æœ‰é™åˆä¼™äºº"
                ],
                "patterns": [
                    r"è®¾ç«‹.*?åŸºé‡‘",
                    r"æˆç«‹.*?åŸºé‡‘",
                    r"æ¨å‡º.*?åŸºé‡‘",
                    r"å¯åŠ¨.*?åŸºé‡‘",
                    r"åŸºé‡‘.*?ä¸‡ç¾å…ƒ",
                    r"åŸºé‡‘.*?åƒä¸‡ç¾å…ƒ",
                    r"åŸºé‡‘.*?äº¿ç¾å…ƒ",
                    r"fund.*?\$.*?million",
                    r"fund.*?\$.*?billion"
                ],
                "weight": 1.0
            },
            "åŸºç¡€è®¾æ–½/é¡¹ç›®ä¸»ç½‘ä¸Šçº¿": {
                "keywords": [
                    "ä¸»ç½‘", "mainnet", "æµ‹è¯•ç½‘", "testnet", "ç½‘ç»œå‡çº§", "ç¡¬åˆ†å‰", "è½¯åˆ†å‰", 
                    "fork", "upgrade", "protocol", "åè®®", "åŒºå—é“¾ç½‘ç»œ", "å…¬é“¾", "ä¾§é“¾", 
                    "Layer 1", "Layer 2", "L1", "L2", "zkEVM", "Rollup", "bridging", 
                    "è·¨é“¾", "äº’æ“ä½œ", "interoperability", "èŠ‚ç‚¹", "node", "éªŒè¯è€…", "validator", 
                    "å…±è¯†", "consensus", "PoS", "PoW", "DPoS", "åˆ†ç‰‡", "sharding", "æ‰©å®¹", 
                    "scaling", "TPS", "ååé‡", "ç½‘ç»œæ€§èƒ½"
                ],
                "patterns": [
                    r"ä¸»ç½‘.*?ä¸Šçº¿",
                    r"ä¸»ç½‘.*?å¯åŠ¨", 
                    r"mainnet.*?launch",
                    r"æµ‹è¯•ç½‘.*?ä¸Šçº¿",
                    r"testnet.*?launch",
                    r"æ­£å¼.*?ä¸Šçº¿",
                    r"å®£å¸ƒ.*?ä¸Šçº¿",
                    r"æˆåŠŸ.*?ä¸Šçº¿"
                ],
                "weight": 1.0
            },
            "DeFi/RWA": {
                "keywords": [
                    "DeFi", "å»ä¸­å¿ƒåŒ–é‡‘è", "decentralized finance", "æµåŠ¨æ€§", "liquidity",
                    "AMM", "è‡ªåŠ¨åšå¸‚å•†", "yield farming", "æµåŠ¨æ€§æŒ–çŸ¿", "è´¨æŠ¼", "staking",
                    "å€Ÿè´·", "lending", "borrowing", "æŠµæŠ¼", "collateral", "TVL", "é”ä»“é‡",
                    "DEX", "å»ä¸­å¿ƒåŒ–äº¤æ˜“æ‰€", "swap", "å…‘æ¢", "æ”¶ç›Šç‡", "APY", "APR",
                    "RWA", "ç°å®ä¸–ç•Œèµ„äº§", "real world assets", "ä»£å¸åŒ–", "tokenization",
                    "èµ„äº§ä»£å¸åŒ–", "å€ºåˆ¸ä»£å¸åŒ–", "æˆ¿åœ°äº§ä»£å¸åŒ–", "å•†å“ä»£å¸åŒ–", "ç¨³å®šå¸",
                    "stablecoin", "USDT", "USDC", "DAI", "algorithmic stablecoin"
                ],
                "patterns": [],
                "weight": 1.0
            },
            "NFT/GameFi/Metaverse": {
                "keywords": [
                    "NFT", "non-fungible token", "æ•°å­—è—å“", "æ•°å­—æ”¶è—å“", "è‰ºæœ¯å“", "å¤´åƒ",
                    "PFP", "profile picture", "OpenSea", "marketplace", "é“¸é€ ", "mint",
                    "GameFi", "åŒºå—é“¾æ¸¸æˆ", "P2E", "play to earn", "è¾¹ç©è¾¹èµš", "æ¸¸æˆä»£å¸",
                    "æ¸¸æˆNFT", "æ¸¸æˆé“å…·", "è™šæ‹ŸåœŸåœ°", "land", "sandbox", "decentraland",
                    "Metaverse", "å…ƒå®‡å®™", "è™šæ‹Ÿä¸–ç•Œ", "virtual world", "VR", "AR",
                    "è™šæ‹Ÿç°å®", "å¢å¼ºç°å®", "æ•°å­—èº«ä»½", "avatar", "è™šæ‹Ÿäºº", "æ•°å­—äºº",
                    "ç¤¾äº¤ä»£å¸", "social token", "åˆ›ä½œè€…ç»æµ", "creator economy"
                ],
                "patterns": [],
                "weight": 1.0
            },
            "äº¤æ˜“æ‰€/é’±åŒ…": {
                "keywords": [
                    "äº¤æ˜“æ‰€", "exchange", "CEX", "ä¸­å¿ƒåŒ–äº¤æ˜“æ‰€", "å¸å®‰", "Binance", "OKX",
                    "Coinbase", "Kraken", "Bybit", "Gate", "Huobi", "FTX", "KuCoin",
                    "ä¸Šå¸", "listing", "ä¸‹æ¶", "delisting", "å……å€¼", "æç°", "deposit", "withdraw",
                    "é’±åŒ…", "wallet", "metamask", "trust wallet", "å†·é’±åŒ…", "çƒ­é’±åŒ…",
                    "ç¡¬ä»¶é’±åŒ…", "software wallet", "ç§é’¥", "private key", "åŠ©è®°è¯", "seed phrase",
                    "å¤šç­¾", "multisig", "custody", "æ‰˜ç®¡", "KYC", "å®åè®¤è¯", "ç›‘ç®¡åˆè§„",
                    "ç‰Œç…§", "license", "åˆè§„", "compliance", "åæ´—é’±", "AML",
                    "çˆ†ä»“", "liquidation", "åˆçº¦", "futures", "æœŸè´§", "æ æ†", "leverage",
                    "å¤šå•", "ç©ºå•", "long", "short", "ä¿è¯é‡‘", "margin", "å¼ºå¹³", "å¼ºåˆ¶å¹³ä»“"
                ],
                "patterns": [],
                "weight": 1.0
            },
            "portfolios": {
                "keywords": [],  # ç”±portfolioæ£€æµ‹é€»è¾‘å•ç‹¬å¤„ç†
                "patterns": [],
                "weight": 0.0
            },
            "å…¶ä»–": {
                "keywords": [],  # å…œåº•åˆ†ç±»ï¼Œæ— ç‰¹å®šå…³é”®è¯
                "patterns": [],
                "weight": 0.0
            }
        }
    
    def preprocess_text(self, text):
        """é¢„å¤„ç†æ–‡æœ¬ï¼šè½¬æ¢ä¸ºå°å†™ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦"""
        if not text:
            return ""
        # ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’ŒåŸºæœ¬æ ‡ç‚¹
        text = re.sub(r'[^\u4e00-\u9fff\w\s\.\,\!\?\-\+\%\$]', ' ', text)
        return text.lower()
    
    def calculate_score(self, text, keywords, patterns):
        """è®¡ç®—æ–‡æœ¬ä¸å…³é”®è¯å’Œæ¨¡å¼çš„åŒ¹é…åˆ†æ•°"""
        text = self.preprocess_text(text)
        score = 0
        
        # è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°
        for keyword in keywords:
            keyword_lower = keyword.lower()
            # è®¡ç®—å…³é”®è¯å‡ºç°æ¬¡æ•°
            count = len(re.findall(r'\b' + re.escape(keyword_lower) + r'\b', text))
            if count == 0:
                # å¯¹äºä¸­æ–‡å…³é”®è¯ï¼Œä½¿ç”¨ç®€å•çš„åŒ…å«åŒ¹é…
                count = text.count(keyword_lower)
            score += count
        
        # è®¡ç®—æ¨¡å¼åŒ¹é…åˆ†æ•°ï¼ˆæƒé‡æ›´é«˜ï¼‰
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            score += len(matches) * 2  # æ¨¡å¼åŒ¹é…ç»™äºˆ2å€æƒé‡
        
        return score

    def check_portfolio_mention(self, article):
        """æ£€æŸ¥æ–‡ç« æ˜¯å¦æåˆ°äº†IOSGæŠ•èµ„ç»„åˆé¡¹ç›®"""
        title = article.get('title', '')
        
        text = self.preprocess_text(title.lower())
        mentioned_projects = []
        for project in self.portfolios:
            pattern = r'\b' + re.escape(project.lower()) + r'\b'
            if re.search(pattern, text):
                mentioned_projects.append(project)
        
        portfolio = len(mentioned_projects) > 0
        
        return {
            'portfolio': portfolio,
            'mentioned_projects': mentioned_projects,
            'mention_count': len(mentioned_projects)
        }

    def classify_article(self, article):
        """å¯¹å•ç¯‡æ–‡ç« è¿›è¡Œåˆ†ç±»"""
        # åˆå¹¶æ ‡é¢˜å’Œå†…å®¹æ–‡æœ¬
        title = article.get('title', '')
        content = article.get('content_text', '')
        combined_text = f"{title} {content}"
        
        scores = {}
        
        # æ£€æŸ¥portfolioå…³é”®è¯
        portfolio_info = self.check_portfolio_mention(article)
        
        # å¦‚æœæ–‡ç« æåˆ°äº†portfolioé¡¹ç›®ï¼Œç›´æ¥åˆ†ç±»ä¸ºportfolios
        if portfolio_info['portfolio']:
            return {
                'category': 'portfolios',
                'confidence': portfolio_info['mention_count'],
                'scores': {'portfolios': portfolio_info['mention_count']},
                'portfolio': portfolio_info['portfolio'],
                'mentioned_projects': portfolio_info['mentioned_projects'],
                'mention_count': portfolio_info['mention_count']
            }
        
        # è®¡ç®—æ¯ä¸ªåˆ†ç±»çš„å¾—åˆ†ï¼ˆæ’é™¤portfolioså’Œå…¶ä»–ï¼‰
        for category, config in self.categories.items():
            if category in ["å…¶ä»–", "portfolios"]:
                continue  # è·³è¿‡å…œåº•åˆ†ç±»å’Œportfoliosåˆ†ç±»
            
            keywords = config['keywords']
            patterns = config.get('patterns', [])
            weight = config['weight']
            score = self.calculate_score(combined_text, keywords, patterns) * weight
            scores[category] = score
        
        # æ‰¾åˆ°æœ€é«˜åˆ†çš„åˆ†ç±»
        if scores and max(scores.values()) > 0:
            best_category = max(scores, key=scores.get)
            confidence = scores[best_category]
        else:
            best_category = "å…¶ä»–"
            confidence = 0
        
        return {
            'category': best_category,
            'confidence': confidence,
            'scores': scores,
            'portfolio': portfolio_info['portfolio'],
            'mentioned_projects': portfolio_info['mentioned_projects'],
            'mention_count': portfolio_info['mention_count']
        }
    
    def classify_articles(self, articles):
        """å¯¹æ‰€æœ‰æ–‡ç« è¿›è¡Œåˆ†ç±»"""
        classified_articles = []
        category_stats = defaultdict(int)
        
        print(f"å¼€å§‹åˆ†ç±» {len(articles)} ç¯‡æ–‡ç« ...")
        
        for i, article in enumerate(articles, 1):
            if i % 100 == 0:
                print(f"å·²å¤„ç† {i}/{len(articles)} ç¯‡æ–‡ç« ")
            
            classification = self.classify_article(article)
            
            # æ·»åŠ åˆ†ç±»ä¿¡æ¯åˆ°æ–‡ç« 
            article_with_class = article.copy()
            article_with_class.update({
                'classification': classification['category'],
                'classification_confidence': classification['confidence'],
                'classification_scores': classification['scores'],
                'portfolio': classification['portfolio'],
                'mentioned_projects': classification['mentioned_projects'],
                'mention_count': classification['mention_count']
            })
            
            classified_articles.append(article_with_class)
            category_stats[classification['category']] += 1
        
        return classified_articles, dict(category_stats)

def main():
    # è¯»å–JSONæ–‡ä»¶
    input_file = "crypto_feeds_unread_20250706_165100.json"
    
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶ {input_file} ä¸å­˜åœ¨")
        return
    except json.JSONDecodeError:
        print(f"âŒ æ–‡ä»¶ {input_file} æ ¼å¼é”™è¯¯")
        return
    
    articles = data.get('articles', [])
    metadata = data.get('metadata', {})
    
    print(f"=== åŠ å¯†è´§å¸æ–‡ç« æ™ºèƒ½åˆ†ç±»å™¨ ===")
    print(f"è¾“å…¥æ–‡ä»¶: {input_file}")
    print(f"æ–‡ç« æ€»æ•°: {len(articles)}")
    print(f"æ¥æºç»Ÿè®¡: {metadata.get('feeds_stats', {})}")
    
    # æ ‡é¢˜å…³é”®è¯ç­›é€‰
    title_keywords = ['ä¸Šçº¿','è½¬ç§»','æ´»åŠ¨','ä¸‹æ¶','24å°æ—¶','24 å°æ—¶','çˆ†ä»“','æ¶¨å¹…','è·Œå¹…','å¥–åŠ±']
    # æ ‡é¢˜å…³é”®è¯ç­›é€‰ - åªä¿ç•™æ ‡é¢˜ä¸åŒ…å«ä»»ä½•å…³é”®è¯çš„æ–‡ç« 
    articles = [
        article for article in articles 
        if not any(keyword.lower() in article.get('title', '').lower() for keyword in title_keywords)
    ]
    
    # æ–‡ç« é•¿åº¦ç­›é€‰
    articles = [
        article for article in articles 
        if len(article.get('content_text', '')) < 300 
    ]
    
    # åˆ›å»ºåˆ†ç±»å™¨å¹¶è¿›è¡Œåˆ†ç±»
    classifier = CryptoArticleClassifier()
    classified_articles, category_stats = classifier.classify_articles(articles)
    
    
    
    
    # æ˜¾ç¤ºåˆ†ç±»ç»Ÿè®¡
    print(f"\n=== åˆ†ç±»ç»“æœç»Ÿè®¡ ===")
    total_classified = len(classified_articles)
    
    for category, count in sorted(category_stats.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / total_classified) * 100
        print(f"ğŸ“Š {category}: {count} ç¯‡ ({percentage:.1f}%)")
    
    # Portfolioç»Ÿè®¡
    portfolio_articles = [a for a in classified_articles if a.get('portfolio', False)]
    print(f"\nâ­ Portfolioé¡¹ç›®ç›¸å…³æ–‡ç« : {len(portfolio_articles)} ç¯‡ ({len(portfolio_articles)/total_classified*100:.1f}%)")
    
    # æŒ‰åˆ†ç±»ç»Ÿè®¡portfolioæ–‡ç« 
    portfolio_by_category = defaultdict(int)
    for article in portfolio_articles:
        portfolio_by_category[article['classification']] += 1
    
    if portfolio_by_category:
        print("   æŒ‰åˆ†ç±»åˆ†å¸ƒ:")
        for category, count in sorted(portfolio_by_category.items(), key=lambda x: x[1], reverse=True):
            print(f"   - {category}: {count} ç¯‡")
    
    # æŒ‰åˆ†ç±»ç»„ç»‡æ–‡ç« 
    articles_by_category = defaultdict(list)
    for article in classified_articles:
        category = article['classification']
        articles_by_category[category].append(article)
    
    # æ˜¾ç¤ºæ¯ä¸ªåˆ†ç±»çš„ç¤ºä¾‹æ–‡ç« 
    print(f"\n=== åˆ†ç±»ç¤ºä¾‹ï¼ˆæ¯ç±»æ˜¾ç¤ºå‰3ç¯‡ï¼‰ ===")
    for category, articles_list in articles_by_category.items():
        print(f"\nğŸ·ï¸ {category} ({len(articles_list)} ç¯‡):")
        
        # æŒ‰ç½®ä¿¡åº¦æ’åºï¼Œæ˜¾ç¤ºå‰3ç¯‡
        sorted_articles = sorted(articles_list, 
                                key=lambda x: x['classification_confidence'], 
                                reverse=True)
        
        for i, article in enumerate(sorted_articles[:3], 1):
            title = article['title']
            source = article['source_feed']
            confidence = article['classification_confidence']
            portfolio_mark = "â­" if article.get('portfolio', False) else ""
            mentioned = article.get('mentioned_projects', [])
            mentioned_str = f" [{', '.join(mentioned[:2])}{'...' if len(mentioned) > 2 else ''}]" if mentioned else ""
            print(f"  {i}. {portfolio_mark}[{source}] {title} (å¾—åˆ†: {confidence}){mentioned_str}")
    
    # ä¿å­˜åˆ†ç±»ç»“æœ
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = f"classified_articles_{timestamp}.json"
    
    output_data = {
        'metadata': {
            **metadata,
            'classification_date': datetime.now().isoformat(),
            'total_articles': len(classified_articles),
            'category_stats': category_stats,
            'categories_defined': list(classifier.categories.keys())
        },
        'articles': classified_articles,
        'articles_by_category': dict(articles_by_category)
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ åˆ†ç±»ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # ä¿å­˜ç®€åŒ–çš„åˆ†ç±»æŠ¥å‘Š
    report_file = f"classification_report_{timestamp}.txt"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("=== åŠ å¯†è´§å¸æ–‡ç« åˆ†ç±»æŠ¥å‘Š ===\n\n")
        f.write(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ–‡ç« æ€»æ•°: {len(classified_articles)}\n\n")
        
        f.write("åˆ†ç±»ç»Ÿè®¡:\n")
        for category, count in sorted(category_stats.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / total_classified) * 100
            f.write(f"  {category}: {count} ç¯‡ ({percentage:.1f}%)\n")
        
        f.write("\nå„åˆ†ç±»çƒ­é—¨æ–‡ç« :\n")
        for category, articles_list in articles_by_category.items():
            f.write(f"\n{category}:\n")
            sorted_articles = sorted(articles_list, 
                                    key=lambda x: x['classification_confidence'], 
                                    reverse=True)
            for i, article in enumerate(sorted_articles[:5], 1):
                f.write(f"  {i}. [{article['source_feed']}] {article['title']}\n")
    
    print(f"ğŸ“„ åˆ†ç±»æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    print(f"\nğŸ¯ åˆ†ç±»å®Œæˆï¼å…±å¤„ç† {len(classified_articles)} ç¯‡æ–‡ç« ")

if __name__ == "__main__":
    main()