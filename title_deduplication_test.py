#!/usr/bin/env python3
"""
æ ‡é¢˜ç›¸ä¼¼åº¦å»é‡æµ‹è¯•è„šæœ¬
å¯¹ formatted txt æ–‡ä»¶ä¸­çš„æ–‡ç« æ ‡é¢˜è¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—ï¼Œç­›é€‰æ‰ç›¸ä¼¼åº¦å¤§äº80%çš„é‡å¤æ–‡ç« 
"""

import re
import difflib
from collections import defaultdict
import argparse
import os


class TitleDeduplicator:
    def __init__(self, similarity_threshold=0.8):
        """
        åˆå§‹åŒ–å»é‡å™¨
        :param similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œé»˜è®¤0.8(80%)
        """
        self.similarity_threshold = similarity_threshold
        
    def clean_title(self, title):
        """
        æ¸…ç†æ ‡é¢˜ï¼Œç§»é™¤å¤šä½™çš„ç¬¦å·å’Œç©ºæ ¼
        """
        # ç§»é™¤é“¾æ¥æ ¼å¼ [title](url)
        title = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', title)
        # ç§»é™¤å¤šä½™ç©ºæ ¼å’Œæ ‡ç‚¹
        title = re.sub(r'\s+', ' ', title).strip()
        # ç§»é™¤å¸¸è§çš„æ–°é—»å‰ç¼€
        title = re.sub(r'^\d+\.\s*', '', title)  # ç§»é™¤åºå·
        return title
        
    def calculate_similarity(self, title1, title2):
        """
        è®¡ç®—ä¸¤ä¸ªæ ‡é¢˜çš„ç›¸ä¼¼åº¦
        ä½¿ç”¨ difflib.SequenceMatcher è®¡ç®—å­—ç¬¦ä¸²ç›¸ä¼¼åº¦
        """
        # æ¸…ç†æ ‡é¢˜
        clean_title1 = self.clean_title(title1).lower()
        clean_title2 = self.clean_title(title2).lower()
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarity = difflib.SequenceMatcher(None, clean_title1, clean_title2).ratio()
        return similarity
        
    def parse_formatted_file(self, file_path):
        """
        è§£æ formatted txt æ–‡ä»¶ï¼Œæå–æ–‡ç« ä¿¡æ¯
        è¿”å›æ ¼å¼: [{'title': '', 'url': '', 'content': '', 'section': ''}, ...]
        """
        articles = []
        current_section = ""
        
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = content.split('\n\n')
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
                
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç« èŠ‚æ ‡é¢˜
            if para.startswith('#'):
                current_section = para.strip('# ').strip()
                continue
                
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ–‡ç« æ¡ç›®ï¼ˆä»¥æ•°å­—å¼€å¤´ï¼‰
            if re.match(r'^\d+\.', para):
                lines = para.split('\n')
                if len(lines) >= 1:
                    # æå–æ ‡é¢˜å’ŒURL
                    title_line = lines[0]
                    url_match = re.search(r'\[([^\]]+)\]\(([^)]+)\)', title_line)
                    
                    if url_match:
                        title = url_match.group(1)
                        url = url_match.group(2)
                        # å†…å®¹æ˜¯å‰©ä½™çš„è¡Œ
                        content = '\n'.join(lines[1:]).strip()
                        
                        articles.append({
                            'title': title,
                            'url': url,
                            'content': content,
                            'section': current_section,
                            'original_text': para
                        })
                        
        return articles
        
    def find_duplicates(self, articles):
        """
        æ‰¾å‡ºç›¸ä¼¼åº¦é«˜äºé˜ˆå€¼çš„é‡å¤æ–‡ç« 
        è¿”å›: (ä¿ç•™çš„æ–‡ç« åˆ—è¡¨, é‡å¤æ–‡ç« ç»„åˆ—è¡¨)
        """
        # ç”¨äºå­˜å‚¨å»é‡åçš„æ–‡ç« 
        unique_articles = []
        # ç”¨äºå­˜å‚¨é‡å¤æ–‡ç« ç»„
        duplicate_groups = []
        # å·²å¤„ç†çš„æ–‡ç« ç´¢å¼•
        processed = set()
        
        for i, article1 in enumerate(articles):
            if i in processed:
                continue
                
            # å½“å‰æ–‡ç« ç»„ï¼ˆåŒ…å«ç›¸ä¼¼çš„æ–‡ç« ï¼‰
            current_group = [article1]
            processed.add(i)
            
            # ä¸åç»­æ–‡ç« æ¯”è¾ƒ
            for j, article2 in enumerate(articles[i+1:], i+1):
                if j in processed:
                    continue
                    
                similarity = self.calculate_similarity(article1['title'], article2['title'])
                
                if similarity >= self.similarity_threshold:
                    current_group.append(article2)
                    processed.add(j)
                    
            # å¦‚æœæœ‰é‡å¤æ–‡ç« ï¼Œè®°å½•åˆ°é‡å¤ç»„ï¼›å¦åˆ™æ·»åŠ åˆ°å”¯ä¸€æ–‡ç« 
            if len(current_group) > 1:
                duplicate_groups.append(current_group)
                # ä¿ç•™ç¬¬ä¸€ç¯‡æ–‡ç« ï¼ˆé€šå¸¸æ˜¯æœ€æ—©å‘å¸ƒçš„ï¼‰
                unique_articles.append(current_group[0])
            else:
                unique_articles.append(current_group[0])
                
        return unique_articles, duplicate_groups
        
    def generate_report(self, original_articles, unique_articles, duplicate_groups):
        """
        ç”Ÿæˆå»é‡æŠ¥å‘Š
        """
        print("=" * 60)
        print("ğŸ“Š æ ‡é¢˜å»é‡åˆ†ææŠ¥å‘Š")
        print("=" * 60)
        
        print(f"åŸå§‹æ–‡ç« æ•°é‡: {len(original_articles)}")
        print(f"å»é‡åæ–‡ç« æ•°é‡: {len(unique_articles)}")
        print(f"é‡å¤æ–‡ç« æ•°é‡: {len(original_articles) - len(unique_articles)}")
        print(f"é‡å¤æ–‡ç« ç»„æ•°: {len(duplicate_groups)}")
        print(f"å»é‡ç‡: {((len(original_articles) - len(unique_articles)) / len(original_articles) * 100):.1f}%")
        
        if duplicate_groups:
            print("\n" + "=" * 60)
            print("ğŸ” é‡å¤æ–‡ç« è¯¦æƒ…")
            print("=" * 60)
            
            for i, group in enumerate(duplicate_groups, 1):
                print(f"\né‡å¤ç»„ {i} (å…±{len(group)}ç¯‡æ–‡ç« ):")
                print("-" * 40)
                
                for j, article in enumerate(group):
                    status = "âœ… ä¿ç•™" if j == 0 else "âŒ åˆ é™¤"
                    section = article.get('section', 'æœªçŸ¥åˆ†ç±»')
                    print(f"{status} [{section}] {article['title']}")
                    
                # æ˜¾ç¤ºç›¸ä¼¼åº¦çŸ©é˜µ
                if len(group) > 1:
                    print("\nç›¸ä¼¼åº¦çŸ©é˜µ:")
                    for k in range(len(group)):
                        for l in range(k+1, len(group)):
                            sim = self.calculate_similarity(group[k]['title'], group[l]['title'])
                            print(f"  æ–‡ç« {k+1} vs æ–‡ç« {l+1}: {sim:.2%}")
                            
        # æŒ‰åˆ†ç±»ç»Ÿè®¡å»é‡æ•ˆæœ
        print("\n" + "=" * 60)
        print("ğŸ“ˆ å„åˆ†ç±»å»é‡ç»Ÿè®¡")
        print("=" * 60)
        
        # ç»Ÿè®¡åŸå§‹æ–‡ç« åˆ†ç±»
        original_by_section = defaultdict(int)
        for article in original_articles:
            original_by_section[article.get('section', 'æœªçŸ¥åˆ†ç±»')] += 1
            
        # ç»Ÿè®¡å»é‡åæ–‡ç« åˆ†ç±»
        unique_by_section = defaultdict(int)
        for article in unique_articles:
            unique_by_section[article.get('section', 'æœªçŸ¥åˆ†ç±»')] += 1
            
        for section in sorted(original_by_section.keys()):
            original_count = original_by_section[section]
            unique_count = unique_by_section[section]
            removed_count = original_count - unique_count
            
            if original_count > 0:
                removal_rate = (removed_count / original_count) * 100
                print(f"{section}: {original_count} â†’ {unique_count} ç¯‡ (åˆ é™¤{removed_count}ç¯‡, {removal_rate:.1f}%)")
                
    def save_deduplicated_file(self, unique_articles, output_path):
        """
        ä¿å­˜å»é‡åçš„æ–‡ä»¶
        """
        # æŒ‰åˆ†ç±»é‡æ–°ç»„ç»‡æ–‡ç« 
        articles_by_section = defaultdict(list)
        for article in unique_articles:
            section = article.get('section', 'æœªçŸ¥åˆ†ç±»')
            articles_by_section[section].append(article)
            
        with open(output_path, 'w', encoding='utf-8') as f:
            global_counter = 1
            
            for section in sorted(articles_by_section.keys()):
                articles = articles_by_section[section]
                if articles:
                    f.write(f"# {section}\n\n")
                    
                    for article in articles:
                        title = article['title']
                        url = article['url']
                        content = article['content']
                        
                        f.write(f"{global_counter}. [{title}]({url})\n\n{content}\n\n")
                        global_counter += 1
                        
        print(f"\nğŸ’¾ å»é‡åçš„æ–‡ä»¶å·²ä¿å­˜åˆ°: {output_path}")


def main():
    parser = argparse.ArgumentParser(description='æ ‡é¢˜ç›¸ä¼¼åº¦å»é‡æµ‹è¯•è„šæœ¬')
    parser.add_argument('input_file', help='è¾“å…¥çš„ formatted txt æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--threshold', '-t', type=float, default=0.8, 
                       help='ç›¸ä¼¼åº¦é˜ˆå€¼ (0-1), é»˜è®¤0.8')
    parser.add_argument('--output', '-o', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ (å¯é€‰)')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.input_file):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.input_file}")
        return
        
    print(f"ğŸ” å¼€å§‹åˆ†ææ–‡ä»¶: {args.input_file}")
    print(f"ğŸ“ ç›¸ä¼¼åº¦é˜ˆå€¼: {args.threshold:.0%}")
    
    # åˆ›å»ºå»é‡å™¨
    deduplicator = TitleDeduplicator(similarity_threshold=args.threshold)
    
    # è§£ææ–‡ä»¶
    print("\nğŸ“– æ­£åœ¨è§£ææ–‡ä»¶...")
    articles = deduplicator.parse_formatted_file(args.input_file)
    print(f"   å‘ç° {len(articles)} ç¯‡æ–‡ç« ")
    
    # æ‰§è¡Œå»é‡
    print("\nğŸ”„ æ­£åœ¨è¿›è¡Œç›¸ä¼¼åº¦åˆ†æ...")
    unique_articles, duplicate_groups = deduplicator.find_duplicates(articles)
    
    # ç”ŸæˆæŠ¥å‘Š
    deduplicator.generate_report(articles, unique_articles, duplicate_groups)
    
    # ä¿å­˜å»é‡åçš„æ–‡ä»¶
    if args.output:
        deduplicator.save_deduplicated_file(unique_articles, args.output)
    else:
        # è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        base_name = os.path.splitext(args.input_file)[0]
        output_file = f"{base_name}_deduplicated.txt"
        deduplicator.save_deduplicated_file(unique_articles, output_file)


if __name__ == "__main__":
    main()