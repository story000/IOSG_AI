#!/usr/bin/env python3
"""
åŸºäºAI APIçš„é¡¹ç›®èèµ„æ–‡ç« ç­›é€‰å™¨ (æ”¯æŒOpenAIå’ŒDeepSeek)
ä»classify_articles.pyçš„ç»“æœä¸­è¿›ä¸€æ­¥ç­›é€‰çœŸå®çš„é¡¹ç›®èèµ„
"""

import json
import openai
import time
from datetime import datetime
import os
import re
import yaml
import difflib

class AIFundingFilter:
    def __init__(self, api_key=None, provider="deepseek"):
        self.provider = provider.lower()
        self.api_key = api_key
        
        # åå°DeepSeek APIå¯†é’¥ (è¯·åœ¨è¿™é‡Œé…ç½®æ‚¨çš„DeepSeek APIå¯†é’¥)
        # æ–¹å¼1: ç›´æ¥åœ¨ä»£ç ä¸­é…ç½®
        # self.deepseek_key = "sk-your-actual-deepseek-key-here"  
        # æ–¹å¼2: ä»ç¯å¢ƒå˜é‡è¯»å–
        self.deepseek_key = os.getenv('DEEPSEEK_API_KEY', 'sk-your-deepseek-key-here')
        
        if self.provider == "deepseek":
            # ä½¿ç”¨DeepSeek API
            if not self.deepseek_key or self.deepseek_key == 'sk-your-deepseek-key-here':
                print("âŒ DeepSeek APIå¯†é’¥æœªé…ç½®")
                print("è¯·åœ¨ai_filter.pyç¬¬21è¡Œé…ç½®æ‚¨çš„DeepSeek APIå¯†é’¥:")
                print('self.deepseek_key = "sk-your-actual-deepseek-key-here"')
                print("æˆ–è®¾ç½®ç¯å¢ƒå˜é‡: export DEEPSEEK_API_KEY=sk-your-key")
                self.api_key = None
                return
            self.api_key = self.deepseek_key
            self.base_url = "https://api.deepseek.com"
            self.model = "deepseek-chat"
            print("âœ… ä½¿ç”¨DeepSeek API")
            print(f"APIå¯†é’¥: {self.api_key[:10]}...")
        else:
            # ä½¿ç”¨OpenAI API
            if api_key:
                self.api_key = api_key
            else:
                self.api_key = os.getenv('OPENAI_API_KEY')
            if not self.api_key:
                print("âŒ è¯·æä¾›OpenAI APIå¯†é’¥")
                return
            self.base_url = None  # OpenAIé»˜è®¤
            self.model = "gpt-3.5-turbo"
            print("âœ… ä½¿ç”¨OpenAI API")
        
        # è®¾ç½®OpenAIå®¢æˆ·ç«¯
        if self.provider == "deepseek":
            openai.api_key = self.api_key
            openai.base_url = self.base_url
        else:
            openai.api_key = self.api_key
        
        
        # ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®
        self.config = self._load_config()
        self.portfolios = self.config.get('portfolio_projects', [])
        self.prompts_config = self.config.get('ai_filter', {})
        
        # æ ‡é¢˜å»é‡ç›¸ä¼¼åº¦é˜ˆå€¼
        self.similarity_threshold = 0.7

    def _load_config(self):
        """ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®"""
        try:
            config_file = "crypto_config.yaml"
            with open(config_file, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            print(f"âœ… å·²åŠ è½½é…ç½®æ–‡ä»¶: {config_file}")
            return config
        except FileNotFoundError:
            print("âŒ æœªæ‰¾åˆ°crypto_config.yamlé…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return {}
        except Exception as e:
            print(f"âŒ åŠ è½½é…ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return {}

    def _format_criteria(self, criteria):
        """æ ¼å¼åŒ–ç­›é€‰æ ‡å‡†ä¸ºå¯è¯»æ–‡æœ¬"""
        formatted = ""
        if 'keep' in criteria:
            for item in criteria['keep']:
                formatted += f"âœ… ä¿ç•™ï¼š{item}\n"
        if 'discard' in criteria:
            for item in criteria['discard']:
                formatted += f"âŒ æŠ›å¼ƒï¼š{item}\n"
        return formatted.strip()

    def _generate_prompt(self, category_name, count, articles_text):
        """æ ¹æ®ç±»åˆ«ç”Ÿæˆprompt"""
        if not self.prompts_config:
            # ä½¿ç”¨æ—§çš„ç¡¬ç¼–ç æ–¹å¼ä½œä¸ºfallback
            return self._get_legacy_prompt(category_name, count, articles_text)
        
        config = self.prompts_config
        
        # æŸ¥æ‰¾å¯¹åº”çš„ç±»åˆ«é…ç½®
        category_config = None
        for cat_name, cat_config in config['categories'].items():
            if cat_name == category_name:
                category_config = cat_config
                break
        
        if not category_config:
            print(f"âš ï¸ æœªæ‰¾åˆ°ç±»åˆ« '{category_name}' çš„é…ç½®ï¼Œä½¿ç”¨é¡¹ç›®èèµ„é…ç½®")
            category_config = config['categories']['é¡¹ç›®èèµ„']
        
        # æ ¼å¼åŒ–ç­›é€‰æ ‡å‡†
        criteria_text = self._format_criteria(category_config['criteria'])
        
        # ç”Ÿæˆå®Œæ•´çš„prompt
        prompt = category_config['prompt_template'].format(
            count=count,
            criteria=criteria_text,
            return_format=config['common']['return_format'],
            empty_return=config['common']['empty_return'],
            instructions=config['common']['instructions'],
            articles=articles_text
        )
        
        return prompt
    
    def _clean_title_for_comparison(self, title):
        """
        æ¸…ç†æ ‡é¢˜ç”¨äºç›¸ä¼¼åº¦æ¯”è¾ƒ
        """
        # ç§»é™¤å¤šä½™ç©ºæ ¼å’Œæ ‡ç‚¹
        title = re.sub(r'\s+', ' ', title).strip()
        # ç§»é™¤å¸¸è§çš„æ–°é—»å‰ç¼€
        title = re.sub(r'^\d+\.\s*', '', title)
        return title.lower()
    
    def _calculate_title_similarity(self, title1, title2):
        """
        è®¡ç®—ä¸¤ä¸ªæ ‡é¢˜çš„ç›¸ä¼¼åº¦
        """
        clean_title1 = self._clean_title_for_comparison(title1)
        clean_title2 = self._clean_title_for_comparison(title2)
        
        similarity = difflib.SequenceMatcher(None, clean_title1, clean_title2).ratio()
        return similarity
    
    def deduplicate_articles_by_title(self, articles, category_name=""):
        """
        æ ¹æ®æ ‡é¢˜ç›¸ä¼¼åº¦å»é‡æ–‡ç« 
        :param articles: æ–‡ç« åˆ—è¡¨
        :param category_name: åˆ†ç±»åç§°ï¼ˆç”¨äºæ—¥å¿—è¾“å‡ºï¼‰
        :return: (å»é‡åçš„æ–‡ç« åˆ—è¡¨, é‡å¤æ–‡ç« ç»Ÿè®¡)
        """
        if len(articles) <= 1:
            return articles, {'removed_count': 0, 'duplicate_groups': []}
        
        # ç”¨äºå­˜å‚¨å»é‡åçš„æ–‡ç« 
        unique_articles = []
        # ç”¨äºå­˜å‚¨é‡å¤æ–‡ç« ç»„
        duplicate_groups = []
        # å·²å¤„ç†çš„æ–‡ç« ç´¢å¼•
        processed = set()
        
        print(f"  ğŸ” å¼€å§‹å¯¹{category_name}è¿›è¡Œæ ‡é¢˜å»é‡ï¼ˆé˜ˆå€¼: {self.similarity_threshold:.0%}ï¼‰...")
        
        for i, article1 in enumerate(articles):
            if i in processed:
                continue
                
            # å½“å‰æ–‡ç« ç»„ï¼ˆåŒ…å«ç›¸ä¼¼çš„æ–‡ç« ï¼‰
            current_group = [article1]
            processed.add(i)
            
            # ä¸åç»­æ–‡ç« æ¯”è¾ƒ
            for j, article2 in enumerate(articles[i+1:], i+1):
                if j in processed:
                    continue
                    
                similarity = self._calculate_title_similarity(
                    article1['title'], article2['title']
                )
                
                if similarity >= self.similarity_threshold:
                    current_group.append(article2)
                    processed.add(j)
                    
            # å¦‚æœæœ‰é‡å¤æ–‡ç« ï¼Œè®°å½•åˆ°é‡å¤ç»„ï¼›å¦åˆ™æ·»åŠ åˆ°å”¯ä¸€æ–‡ç« 
            if len(current_group) > 1:
                duplicate_groups.append(current_group)
                # ä¿ç•™ç¬¬ä¸€ç¯‡æ–‡ç« ï¼ˆé€šå¸¸æ˜¯æœ€æ—©å‘å¸ƒçš„ï¼‰
                unique_articles.append(current_group[0])
                
                # è¾“å‡ºé‡å¤ä¿¡æ¯
                print(f"    ğŸ“ å‘ç°é‡å¤ç»„ï¼ˆ{len(current_group)}ç¯‡ï¼‰:")
                for k, article in enumerate(current_group):
                    status = "âœ… ä¿ç•™" if k == 0 else "âŒ åˆ é™¤"
                    title_preview = article['title'][:50] + "..." if len(article['title']) > 50 else article['title']
                    print(f"      {status} {title_preview}")
                    
            else:
                unique_articles.append(current_group[0])
                
        removed_count = len(articles) - len(unique_articles)
        removal_rate = (removed_count / len(articles)) * 100 if len(articles) > 0 else 0
        
        print(f"  ğŸ“Š {category_name}å»é‡å®Œæˆ: {len(articles)} â†’ {len(unique_articles)} ç¯‡ (åˆ é™¤{removed_count}ç¯‡, {removal_rate:.1f}%)")
        
        return unique_articles, {
            'removed_count': removed_count,
            'duplicate_groups': duplicate_groups,
            'removal_rate': removal_rate
        }
    
    def cross_category_deduplication(self, filtered_results):
        """
        è·¨æ¿å—å»é‡ï¼šå¦‚æœæ–‡ç« åœ¨é«˜ä¼˜å…ˆçº§æ¿å—å‡ºç°ï¼Œåˆ™ä»ä½ä¼˜å…ˆçº§æ¿å—åˆ é™¤
        ä¼˜å…ˆçº§ï¼šPortfolio > é¡¹ç›®èèµ„ > åŸºé‡‘èèµ„ > å…¶ä»–æ¿å—
        """
        # å®šä¹‰ä¼˜å…ˆçº§é¡ºåº
        priority_categories = [
            ("portfolio", "Portfolio"),
            ("project", "é¡¹ç›®èèµ„"), 
            ("fund", "åŸºé‡‘èèµ„"),
            ("blockchain", "å…¬é“¾/L2/ä¸»ç½‘"),
            ("middleware", "ä¸­é—´ä»¶/å·¥å…·åè®®"),
            ("defi", "DeFi"),
            ("rwa", "RWA"),
            ("nft", "NFT"),
            ("gamefi", "GameFi"),
            ("metaverse", "Metaverse/Web3ç¤¾äº¤"),
            ("exchange_wallet", "äº¤æ˜“æ‰€/é’±åŒ…"),
            ("ai_crypto", "AI + Crypto"),
            ("depin", "DePIN")
        ]
        
        # å­˜å‚¨å·²ç»åœ¨é«˜ä¼˜å…ˆçº§æ¿å—å‡ºç°çš„æ–‡ç« æ ‡é¢˜
        seen_titles = set()
        cross_dedup_stats = {}
        
        # æŒ‰ä¼˜å…ˆçº§å¤„ç†æ¯ä¸ªæ¿å—
        for category_key, category_name in priority_categories:
            articles = filtered_results.get(category_key, [])
            if not articles:
                cross_dedup_stats[category_name] = {'removed_count': 0}
                continue
                
            # å½“å‰æ¿å—çš„æ–‡ç« 
            current_titles = set()
            remaining_articles = []
            removed_count = 0
            
            for article in articles:
                article_title_clean = self._clean_title_for_comparison(article['title'])
                
                # æ£€æŸ¥æ˜¯å¦ä¸å·²å¤„ç†çš„é«˜ä¼˜å…ˆçº§æ¿å—æ–‡ç« é‡å¤
                is_duplicate = False
                for seen_title in seen_titles:
                    similarity = difflib.SequenceMatcher(None, article_title_clean, seen_title).ratio()
                    if similarity >= self.similarity_threshold:
                        is_duplicate = True
                        break
                
                if is_duplicate:
                    removed_count += 1
                    print(f"  âŒ ä»{category_name}åˆ é™¤é‡å¤æ–‡ç« : {article['title'][:50]}...")
                else:
                    remaining_articles.append(article)
                    current_titles.add(article_title_clean)
            
            # æ›´æ–°ç»“æœ
            filtered_results[category_key] = remaining_articles
            cross_dedup_stats[category_name] = {'removed_count': removed_count}
            
            # å°†å½“å‰æ¿å—çš„æ ‡é¢˜åŠ å…¥å·²è§æ ‡é¢˜é›†åˆ
            seen_titles.update(current_titles)
            
            if removed_count > 0:
                original_count = len(articles)
                remaining_count = len(remaining_articles)
                print(f"  ğŸ“Š {category_name}è·¨æ¿å—å»é‡: {original_count} â†’ {remaining_count} ç¯‡ (åˆ é™¤{removed_count}ç¯‡)")
        
        # ç»Ÿè®¡æ€»çš„è·¨æ¿å—å»é‡æƒ…å†µ
        total_cross_removed = sum(stats['removed_count'] for stats in cross_dedup_stats.values())
        if total_cross_removed > 0:
            print(f"ğŸ“Š è·¨æ¿å—å»é‡å®Œæˆï¼Œå…±åˆ é™¤ {total_cross_removed} ç¯‡é‡å¤æ–‡ç« ")
        else:
            print(f"ğŸ“Š è·¨æ¿å—å»é‡å®Œæˆï¼Œæœªå‘ç°é‡å¤æ–‡ç« ")
            
        return filtered_results, cross_dedup_stats

    def _get_legacy_prompt(self, category_name, count, articles_text):
        """æ—§çš„ç¡¬ç¼–ç promptæ–¹å¼ï¼ˆä½œä¸ºfallbackï¼‰"""
        # è¿™é‡Œå¯ä»¥ä¿ç•™åŸæ¥çš„prompté€»è¾‘ä½œä¸ºå¤‡ç”¨
        legacy_prompt = f"""è¯·åˆ†æä»¥ä¸‹{count}ç¯‡åŠ å¯†è´§å¸æ–°é—»ï¼Œåˆ¤æ–­å“ªäº›æ˜¯çœŸå®çš„{category_name}æ–°é—»ã€‚
        
è¯·åªè¿”å›ç¬¦åˆæ¡ä»¶çš„æ–‡ç« IDï¼Œæ ¼å¼ï¼š[id1, id2, id3]
å¦‚æœæ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æ–‡ç« ï¼Œè¿”å›ï¼š[]

æ–‡ç« åˆ—è¡¨ï¼š
{articles_text}"""
        return legacy_prompt

    def filter_batch_articles(self, articles_batch, article_type="project"):
        """æ‰¹å¤„ç†ç­›é€‰æ–‡ç« """
        # ä¸ºæ¯ç¯‡æ–‡ç« ç”Ÿæˆç®€çŸ­ID
        articles_text = ""
        id_mapping = {}
        
        for i, article in enumerate(articles_batch):
            article_id = f"ID{i+1}"
            title = article.get('title', '')
            content = article.get('content_text', '')[:100]
            id_mapping[article_id] = i
            if article_type in ["project", "blockchain", "middleware", "defi", "rwa", "nft", "gamefi", "metaverse", "exchange_wallet", "ai_crypto", "depin"]:
                articles_text += f"\n{article_id}: {title}\n"
            else:  # fund and others
                articles_text += f"\n{article_id}: {title}  - {content} \n"
        
        # æ˜ å°„æ–‡ç« ç±»å‹åˆ°ä¸­æ–‡ç±»åˆ«åç§°
        type_to_category = {
            "project": "é¡¹ç›®èèµ„",
            "fund": "åŸºé‡‘èèµ„",
            "blockchain": "å…¬é“¾/L2/ä¸»ç½‘",
            "middleware": "ä¸­é—´ä»¶/å·¥å…·åè®®",
            "defi": "DeFi",
            "rwa": "RWA",
            "nft": "NFT",
            "gamefi": "GameFi",
            "metaverse": "Metaverse/Web3ç¤¾äº¤",
            "exchange_wallet": "äº¤æ˜“æ‰€/é’±åŒ…",
            "ai_crypto": "AI + Crypto",
            "depin": "DePIN"
        }
        
        category_name = type_to_category.get(article_type, "é¡¹ç›®èèµ„")
        
        # ä½¿ç”¨æ–°çš„promptç”Ÿæˆæ–¹æ³•
        prompt = self._generate_prompt(category_name, len(articles_batch), articles_text)
            
        
        try:
            # æ ¹æ®æä¾›å•†åˆ›å»ºå®¢æˆ·ç«¯
            if self.provider == "deepseek":
                client = openai.OpenAI(
                    api_key=self.api_key,
                    base_url=self.base_url
                )
            else:
                client = openai.OpenAI(api_key=self.api_key)
                
            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=200,
            )
            
            result = response.choices[0].message.content.strip()
            print(f"  ğŸ¤– AIè¿”å›: {result}")
            import sys
            sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¾“å‡º
            
            # è§£æè¿”å›çš„IDåˆ—è¡¨
            import re
            # Extract IDs from the AI response
            raw_ids = re.findall(r'\d+', result)
            # Add 'ID' prefix to each number
            id_matches = ['ID' + num for num in raw_ids]
            
            # è½¬æ¢ä¸ºæ–‡ç« ç´¢å¼•
            selected_indices = []
            for id_str in id_matches:
                if id_str in id_mapping:
                    selected_indices.append(id_mapping[id_str])
            
            return {
                'selected_indices': selected_indices,
                'ai_response': result,
                'success': True
            }
            
        except Exception as e:
            print(f"  âŒ APIè°ƒç”¨å¤±è´¥: {e}")
            return {
                'selected_indices': [],
                'ai_response': f"APIé”™è¯¯: {str(e)}",
                'success': False
            }
    
    def batch_filter(self, articles, batch_size=20, max_articles=None, article_type="project"):
        """æ‰¹é‡ç­›é€‰æ–‡ç« """
        if max_articles:
            articles = articles[:max_articles]
        
        type_name = "é¡¹ç›®èèµ„" if article_type == "project" else "åŸºé‡‘èèµ„"
        print(f"å¼€å§‹ä½¿ç”¨OpenAI APIæ‰¹é‡ç­›é€‰ {len(articles)} ç¯‡{type_name}æ–‡ç« ...")
        print(f"ğŸ“¦ æ‰¹å¤„ç†å¤§å°: {batch_size} ç¯‡/æ¬¡")
        
        # è®¡ç®—é¢„ä¼°æˆæœ¬
        num_batches = (len(articles) + batch_size - 1) // batch_size
        print(f"ğŸ“Š é¢„è®¡APIè°ƒç”¨æ¬¡æ•°: {num_batches} æ¬¡")
        print("âš ï¸ æ³¨æ„ï¼šè¿™å°†æ¶ˆè€—APIè°ƒç”¨æ¬¡æ•°")
        
        # è¯¢é—®ç¡®è®¤
        confirm = input(f"ç¡®è®¤å¤„ç†{len(articles)}ç¯‡{type_name}æ–‡ç« ï¼Ÿ(y/n): ").strip().lower()
        if confirm != 'y':
            print("å–æ¶ˆå¤„ç†")
            return []
        
        filtered_articles = []
        
        # åˆ†æ‰¹å¤„ç†
        for batch_start in range(0, len(articles), batch_size):
            batch_end = min(batch_start + batch_size, len(articles))
            batch_articles = articles[batch_start:batch_end]
            batch_num = (batch_start // batch_size) + 1
            
            # è®¡ç®—å¹¶æ˜¾ç¤ºè¿›åº¦
            progress = int((batch_num / num_batches) * 100)
            print(f"\nğŸ“¦ å¤„ç†{type_name}æ‰¹æ¬¡ {batch_num}/{num_batches} ({len(batch_articles)} ç¯‡æ–‡ç« ) - è¿›åº¦: {progress}%")
            print(f"   æ–‡ç«  {batch_start+1}-{batch_end}")
            import sys
            sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¿›åº¦è¾“å‡º
            
            # æ˜¾ç¤ºå½“å‰æ‰¹æ¬¡æ–‡ç« æ ‡é¢˜
            for i, article in enumerate(batch_articles):
                print(f"   ID{i+1}: {article['title'][:60]}")
            sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°æ–‡ç« åˆ—è¡¨
            
            # è°ƒç”¨APIæ‰¹é‡ç­›é€‰ï¼Œä¼ å…¥æ–‡ç« ç±»å‹
            filter_result = self.filter_batch_articles(batch_articles, article_type)
            
            if filter_result['success']:
                selected_indices = filter_result['selected_indices']
                print(f"  âœ… é€‰ä¸­ {len(selected_indices)} ç¯‡: {selected_indices}")
                sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°ç»“æœè¾“å‡º
                
                # æ·»åŠ é€‰ä¸­çš„æ–‡ç« 
                for idx in selected_indices:
                    if 0 <= idx < len(batch_articles):
                        article_with_filter = batch_articles[idx].copy()
                        
                        
                        article_with_filter['ai_filter'] = {
                            'keep': True,
                            'reason': filter_result['ai_response'],
                            'batch_processed': True,
                            'batch_id': batch_num,
                            'article_type': article_type
                        }
                        
                        
                        filtered_articles.append(article_with_filter)
            else:
                print(f"  âŒ æ‰¹æ¬¡å¤„ç†å¤±è´¥")
            
            # é¿å…APIé€Ÿç‡é™åˆ¶
            if batch_num < num_batches:
                print(f"  â³ æš‚åœ1ç§’...")
                time.sleep(1)
        
        print(f"\nğŸ¯ {type_name}æ‰¹é‡ç­›é€‰å®Œæˆï¼")
        print(f"åŸå§‹æ–‡ç« : {len(articles)} ç¯‡")
        print(f"ç­›é€‰å: {len(filtered_articles)} ç¯‡")
        print(f"è¿‡æ»¤ç‡: {(1 - len(filtered_articles)/len(articles))*100:.1f}%")
        print(f"APIè°ƒç”¨æ¬¡æ•°: {num_batches} æ¬¡")
        
        return filtered_articles

def main():
    # è¯»å–æœ€æ–°çš„åˆ†ç±»ç»“æœæ–‡ä»¶
    latest_file = "latest_classified.json"
    
    # å¦‚æœæœ€æ–°æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•æŸ¥æ‰¾æ—§æ ¼å¼çš„æ–‡ä»¶
    if not os.path.exists(latest_file):
        print("âŒ æœªæ‰¾åˆ°åˆ†ç±»ç»“æœæ–‡ä»¶")
        print("è¯·å…ˆè¿è¡Œ python classify_articles.py")
        return
    
    print(f"è¯»å–æ–‡ä»¶: {latest_file}")
    
    try:
        with open(latest_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return
    
    # è·å–å„ç±»æ–‡ç« 
    articles_by_category = data.get('articles_by_category', {})
    project_funding_articles = articles_by_category.get('é¡¹ç›®èèµ„', [])
    fund_funding_articles = articles_by_category.get('åŸºé‡‘èèµ„', [])
    blockchain_articles = articles_by_category.get('å…¬é“¾/L2/ä¸»ç½‘', [])
    middleware_articles = articles_by_category.get('ä¸­é—´ä»¶/å·¥å…·åè®®', [])
    defi_articles = articles_by_category.get('DeFi', [])
    rwa_articles = articles_by_category.get('RWA', [])
    nft_articles = articles_by_category.get('NFT', [])
    gamefi_articles = articles_by_category.get('GameFi', [])
    metaverse_articles = articles_by_category.get('Metaverse/Web3ç¤¾äº¤', [])
    exchange_wallet_articles = articles_by_category.get('äº¤æ˜“æ‰€/é’±åŒ…', [])
    ai_crypto_articles = articles_by_category.get('AI + Crypto', [])
    depin_articles = articles_by_category.get('DePIN', [])
    portfolio_articles = articles_by_category.get('portfolios', [])
    
    # è®¡ç®—æ€»æ–‡ç« æ•°
    total_articles = (len(project_funding_articles) + len(fund_funding_articles) + 
                     len(blockchain_articles) + len(middleware_articles) + 
                     len(defi_articles) + len(rwa_articles) + len(nft_articles) +
                     len(gamefi_articles) + len(metaverse_articles) + 
                     len(exchange_wallet_articles) + len(ai_crypto_articles) + 
                     len(depin_articles) + len(portfolio_articles))
    
    if total_articles == 0:
        print("âŒ æœªæ‰¾åˆ°å¯ç­›é€‰çš„æ–‡ç« ")
        return
    
    print(f"=== AI æ–‡ç« ç­›é€‰å™¨ ===")
    print(f"æ‰¾åˆ°é¡¹ç›®èèµ„åˆ†ç±»æ–‡ç« : {len(project_funding_articles)} ç¯‡")
    print(f"æ‰¾åˆ°åŸºé‡‘èèµ„åˆ†ç±»æ–‡ç« : {len(fund_funding_articles)} ç¯‡")
    print(f"æ‰¾åˆ°å…¬é“¾/L2/ä¸»ç½‘æ–‡ç« : {len(blockchain_articles)} ç¯‡")
    print(f"æ‰¾åˆ°ä¸­é—´ä»¶/å·¥å…·åè®®æ–‡ç« : {len(middleware_articles)} ç¯‡")
    print(f"æ‰¾åˆ°DeFiæ–‡ç« : {len(defi_articles)} ç¯‡")
    print(f"æ‰¾åˆ°RWAæ–‡ç« : {len(rwa_articles)} ç¯‡")
    print(f"æ‰¾åˆ°NFTæ–‡ç« : {len(nft_articles)} ç¯‡")
    print(f"æ‰¾åˆ°GameFiæ–‡ç« : {len(gamefi_articles)} ç¯‡")
    print(f"æ‰¾åˆ°Metaverse/Web3ç¤¾äº¤æ–‡ç« : {len(metaverse_articles)} ç¯‡")
    print(f"æ‰¾åˆ°äº¤æ˜“æ‰€/é’±åŒ…æ–‡ç« : {len(exchange_wallet_articles)} ç¯‡")
    print(f"æ‰¾åˆ°AI + Cryptoæ–‡ç« : {len(ai_crypto_articles)} ç¯‡")
    print(f"æ‰¾åˆ°DePINæ–‡ç« : {len(depin_articles)} ç¯‡")
    print(f"æ‰¾åˆ°Portfolioæ–‡ç« : {len(portfolio_articles)} ç¯‡")
    
    # é€‰æ‹©APIæä¾›å•†
    print(f"\n=== é€‰æ‹©AI APIæä¾›å•† ===")
    print("1. DeepSeek API (åå°é…ç½®)")
    print("2. OpenAI API (ç”¨æˆ·è¾“å…¥)")
    
    provider_choice = input("è¯·é€‰æ‹©APIæä¾›å•† (1/2ï¼Œé»˜è®¤1): ").strip()
    
    api_key = None
    provider = "deepseek"
    
    if provider_choice == "2":
        provider = "openai"
        api_key = input("è¯·è¾“å…¥OpenAI APIå¯†é’¥: ").strip()
        if not api_key:
            print("âŒ æœªæä¾›OpenAI APIå¯†é’¥")
            return
    else:
        print("âœ… ä½¿ç”¨åå°DeepSeek API")
    
    # åˆ›å»ºç­›é€‰å™¨
    filter = AIFundingFilter(api_key, provider)
    if not filter.api_key:
        print("âŒ APIå¯†é’¥é…ç½®å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
        return
    
    # è¯¢é—®å¤„ç†æ•°é‡å’Œæ‰¹å¤§å°
    max_articles = input(f"è¦å¤„ç†å¤šå°‘ç¯‡æ–‡ç« ï¼Ÿ(é»˜è®¤å…¨éƒ¨{total_articles}ç¯‡ï¼Œè¾“å…¥æ•°å­—é™åˆ¶æ•°é‡): ").strip()
    if max_articles.isdigit():
        max_articles = int(max_articles)
    else:
        max_articles = None
    
    batch_size = input("æ‰¹å¤„ç†å¤§å°ï¼Ÿ(é»˜è®¤20ç¯‡/æ¬¡ï¼Œè¾“å…¥æ•°å­—ä¿®æ”¹): ").strip()
    if batch_size.isdigit():
        batch_size = int(batch_size)
    else:
        batch_size = 20
    
    # å®šä¹‰æ‰€æœ‰ç±»åˆ«çš„é…ç½®
    categories_config = [
        ("é¡¹ç›®èèµ„", project_funding_articles, "project"),
        ("åŸºé‡‘èèµ„", fund_funding_articles, "fund"),
        ("å…¬é“¾/L2/ä¸»ç½‘", blockchain_articles, "blockchain"),
        ("ä¸­é—´ä»¶/å·¥å…·åè®®", middleware_articles, "middleware"),
        ("DeFi", defi_articles, "defi"),
        ("RWA", rwa_articles, "rwa"),
        ("NFT", nft_articles, "nft"),
        ("GameFi", gamefi_articles, "gamefi"),
        ("Metaverse/Web3ç¤¾äº¤", metaverse_articles, "metaverse"),
        ("äº¤æ˜“æ‰€/é’±åŒ…", exchange_wallet_articles, "exchange_wallet"),
        ("AI + Crypto", ai_crypto_articles, "ai_crypto"),
        ("DePIN", depin_articles, "depin")
    ]
    
    # ç»Ÿä¸€ç­›é€‰å„ç±»æ–‡ç« 
    filtered_results = {}
    dedup_stats = {}
    
    for category_name, articles, article_type in categories_config:
        if articles:
            print(f"\nğŸ¯ å¼€å§‹ç­›é€‰{category_name}æ–‡ç« ...")
            # å…ˆè¿›è¡ŒAIç­›é€‰
            ai_filtered = filter.batch_filter(articles, batch_size, max_articles, article_type)
            
            # å†è¿›è¡Œæ ‡é¢˜å»é‡
            if ai_filtered:
                print(f"\nğŸ”„ å¯¹{category_name}è¿›è¡Œæ ‡é¢˜å»é‡...")
                deduplicated, dedup_stat = filter.deduplicate_articles_by_title(ai_filtered, category_name)
                filtered_results[article_type] = deduplicated
                dedup_stats[category_name] = dedup_stat
            else:
                filtered_results[article_type] = []
                dedup_stats[category_name] = {'removed_count': 0, 'duplicate_groups': [], 'removal_rate': 0}
        else:
            filtered_results[article_type] = []
            dedup_stats[category_name] = {'removed_count': 0, 'duplicate_groups': [], 'removal_rate': 0}
    
    # Portfolioæ–‡ç« ä¸éœ€è¦AIç­›é€‰ï¼Œä½†éœ€è¦å»é‡
    if portfolio_articles:
        print(f"\nâ­ Portfolioæ–‡ç« æ— éœ€AIç­›é€‰ï¼Œç›´æ¥è¿›è¡Œæ ‡é¢˜å»é‡...")
        deduplicated_portfolio, portfolio_dedup_stat = filter.deduplicate_articles_by_title(portfolio_articles, "Portfolio")
        filtered_results["portfolio"] = deduplicated_portfolio
        dedup_stats["Portfolio"] = portfolio_dedup_stat
    else:
        filtered_results["portfolio"] = []
        dedup_stats["Portfolio"] = {'removed_count': 0, 'duplicate_groups': [], 'removal_rate': 0}
    
    # è·¨æ¿å—å»é‡ï¼šä¼˜å…ˆçº§ Portfolio > é¡¹ç›®èèµ„ > åŸºé‡‘èèµ„ > å…¶ä»–æ¿å—
    print(f"\nğŸ”„ æ‰§è¡Œè·¨æ¿å—å»é‡...")
    filtered_results, cross_dedup_stats = filter.cross_category_deduplication(filtered_results)
    

    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
 
    # ç”Ÿæˆæ ¼å¼åŒ–è¾“å‡º
    formatted_output_file = f"formatted_report_{timestamp}.txt"
    with open(formatted_output_file, 'w', encoding='utf-8') as f:
        def clean_content(content):
            import re
            # åˆ é™¤å„ç§æ–°é—»æ¥æºå‰ç¼€ï¼ŒåŒ…æ‹¬å¸¦ç©ºæ ¼çš„æ¨¡å¼
            content = re.sub(r'.*?æ¶ˆæ¯[ï¼Œ,]\s*', '', content)
            content = re.sub(r'.*?æŠ¥é“[ï¼Œ,]\s*', '', content)
            content = re.sub(r'æ·±æ½®\s*TechFlow\s*[æ¶ˆæ¯æŠ¥é“]*[ï¼Œ,]\s*', '', content)
            content = re.sub(r'TechFlow\s*æ·±æ½®\s*[æ¶ˆæ¯æŠ¥é“]*[ï¼Œ,]\s*', '', content)
            content = re.sub(r'PANews\s*\d+æœˆ\d+æ—¥æ¶ˆæ¯[ï¼Œ,]\s*', '', content)
            content = re.sub(r'Wu\s*Blockchain\s*[æ¶ˆæ¯æŠ¥é“]*[ï¼Œ,]\s*', '', content)
            content = re.sub(r'Cointelegraph\s*ä¸­æ–‡\s*[æ¶ˆæ¯æŠ¥é“]*[ï¼Œ,]\s*', '', content)
            # åˆ é™¤å¤šä½™çš„ç©ºç™½
            content = re.sub(r'\s+', ' ', content).strip()
            return content
        
        def delete_reports(articles):
            new_articles = []
            for article in articles:
                if not re.search(r"å‘¨æŠ¥|æ—¥æŠ¥|æœˆæŠ¥", article['title']) or len(article.get('content_text', '')) > 200:
                    new_articles.append(article)
            return new_articles

        # ä½¿ç”¨å…¨å±€ç¼–å·
        global_counter = 1
        
        # ä½¿ç”¨å¾ªç¯å¤„ç†æ‰€æœ‰åˆ†ç±»çš„è¾“å‡º
        output_sections = [
            ("é¡¹ç›®èèµ„", filtered_results.get("project", []), "2. é¡¹ç›®èèµ„ä»‹ç»"),
            ("åŸºé‡‘èèµ„", filtered_results.get("fund", []), "3. åŸºé‡‘èèµ„ä»‹ç»"),
            ("å…¬é“¾/L2/ä¸»ç½‘", filtered_results.get("blockchain", []), "4. å…¬é“¾/L2/ä¸»ç½‘"),
            ("ä¸­é—´ä»¶/å·¥å…·åè®®", filtered_results.get("middleware", []), "5. ä¸­é—´ä»¶/å·¥å…·åè®®"),
            ("DeFi", filtered_results.get("defi", []), "6. DeFi"),
            ("RWA", filtered_results.get("rwa", []), "7. RWA"),
            ("NFT", filtered_results.get("nft", []), "8. NFT"),
            ("GameFi", filtered_results.get("gamefi", []), "9. GameFi"),
            ("Metaverse/Web3ç¤¾äº¤", filtered_results.get("metaverse", []), "10. Metaverse/Web3ç¤¾äº¤"),
            ("äº¤æ˜“æ‰€/é’±åŒ…", filtered_results.get("exchange_wallet", []), "11. äº¤æ˜“æ‰€/é’±åŒ…"),
            ("AI + Crypto", filtered_results.get("ai_crypto", []), "12. AI + Crypto"),
            ("DePIN", filtered_results.get("depin", []), "13. DePIN")
        ]
        
        for category_name, articles_list, section_title in output_sections:
            if articles_list:
                f.write(f"# {section_title}\n\n")
                for article in delete_reports(articles_list):
                    title = article['title']
                    url = article['url']
                    content = clean_content(article.get('content_text', ''))
                    
                    f.write(f"{global_counter}. [{title}]({url})\n\n{content}\n\n")
                    global_counter += 1
        
        # Portfolioæ¿å—
        if filtered_results["portfolio"]:
            f.write("# 14. Our portfolio (è¿™é‡Œæ ‡çº¢çš„åœ¨å…¬ä¼—å·ç¼–è¾‘æ—¶å¯¹åº”æ ‡çº¢å³å¯)\n\n")
            for article in delete_reports(filtered_results["portfolio"]):
                title = article['title']
                url = article['url']
                content = clean_content(article.get('content_text', ''))
                
                f.write(f"{global_counter}. [{title}]({url})\n\n{content}\n\n")
                global_counter += 1
    
    print(f"ğŸ“‹ æ ¼å¼åŒ–è¾“å‡ºå·²ä¿å­˜åˆ°: {formatted_output_file}")
    print(f"\nğŸ¯ AIç­›é€‰ + æ ‡é¢˜å»é‡ + è·¨æ¿å—å»é‡å®Œæˆï¼")
    
    # ç»Ÿè®¡è¾“å‡ºï¼Œä½¿ç”¨å¾ªç¯
    stats_config = [
        ("é¡¹ç›®èèµ„", project_funding_articles, "project"),
        ("åŸºé‡‘èèµ„", fund_funding_articles, "fund"),
        ("å…¬é“¾/L2/ä¸»ç½‘", blockchain_articles, "blockchain"),
        ("ä¸­é—´ä»¶/å·¥å…·åè®®", middleware_articles, "middleware"),
        ("DeFi", defi_articles, "defi"),
        ("RWA", rwa_articles, "rwa"),
        ("NFT", nft_articles, "nft"),
        ("GameFi", gamefi_articles, "gamefi"),
        ("Metaverse/Web3ç¤¾äº¤", metaverse_articles, "metaverse"),
        ("äº¤æ˜“æ‰€/é’±åŒ…", exchange_wallet_articles, "exchange_wallet"),
        ("AI + Crypto", ai_crypto_articles, "ai_crypto"),
        ("DePIN", depin_articles, "depin"),
        ("Portfolio", portfolio_articles, "portfolio")
    ]
    
    for category_name, original_articles, result_key in stats_config:
        filtered_count = len(filtered_results.get(result_key, []))
        dedup_removed = dedup_stats.get(category_name, {}).get('removed_count', 0)
        cross_removed = cross_dedup_stats.get(category_name, {}).get('removed_count', 0)
        
        removal_info = []
        if dedup_removed > 0:
            removal_info.append(f"æ ‡é¢˜å»é‡{dedup_removed}ç¯‡")
        if cross_removed > 0:
            removal_info.append(f"è·¨æ¿å—å»é‡{cross_removed}ç¯‡")
            
        if removal_info:
            removal_str = f" ({', '.join(removal_info)})"
            print(f"   {category_name}: {len(original_articles)} â†’ {filtered_count} ç¯‡{removal_str}")
        else:
            print(f"   {category_name}: {len(original_articles)} â†’ {filtered_count} ç¯‡")
    
    # è®¡ç®—æ€»è®¡
    total_original = sum(len(articles) for _, articles, _ in stats_config)
    total_filtered = sum(len(filtered_results.get(result_key, [])) for _, _, result_key in stats_config)
    total_dedup_removed = sum(dedup_stats.get(cat_name, {}).get('removed_count', 0) for cat_name, _, _ in stats_config)
    total_cross_removed = sum(cross_dedup_stats.get(cat_name, {}).get('removed_count', 0) for cat_name, _, _ in stats_config)
    
    print(f"   æ€»è®¡: {total_original} â†’ {total_filtered} ç¯‡")
    
    if total_dedup_removed > 0 or total_cross_removed > 0:
        print(f"\nğŸ“Š å»é‡ç»Ÿè®¡æ±‡æ€»:")
        if total_dedup_removed > 0:
            print(f"   æ ‡é¢˜å»é‡åˆ é™¤: {total_dedup_removed} ç¯‡ ({(total_dedup_removed / total_original * 100):.1f}%)")
        if total_cross_removed > 0:
            print(f"   è·¨æ¿å—å»é‡åˆ é™¤: {total_cross_removed} ç¯‡ ({(total_cross_removed / total_original * 100):.1f}%)")
        print(f"   æ€»å»é‡åˆ é™¤: {total_dedup_removed + total_cross_removed} ç¯‡ ({((total_dedup_removed + total_cross_removed) / total_original * 100):.1f}%)")
    
    
    # åœ¨æ§åˆ¶å°æ˜¾ç¤ºæ ¼å¼åŒ–è¾“å‡ºé¢„è§ˆ
    print(f"\n=== æ ¼å¼åŒ–è¾“å‡ºé¢„è§ˆ ===")
    
    def clean_content_preview(content):
        import re
        content = re.sub(r'.*?æ¶ˆæ¯[ï¼Œ,]\s*', '', content)
        content = re.sub(r'.*?æŠ¥é“[ï¼Œ,]\s*', '', content)
        content = re.sub(r'æ·±æ½®\s*TechFlow\s*[æ¶ˆæ¯æŠ¥é“]*[ï¼Œ,]\s*', '', content)
        content = re.sub(r'TechFlow\s*æ·±æ½®\s*[æ¶ˆæ¯æŠ¥é“]*[ï¼Œ,]\s*', '', content)
        content = re.sub(r'PANews\s*\d+æœˆ\d+æ—¥æ¶ˆæ¯[ï¼Œ,]\s*', '', content)
        content = re.sub(r'Wu\s*Blockchain\s*[æ¶ˆæ¯æŠ¥é“]*[ï¼Œ,]\s*', '', content)
        content = re.sub(r'Cointelegraph\s*ä¸­æ–‡\s*[æ¶ˆæ¯æŠ¥é“]*[ï¼Œ,]\s*', '', content)
        content = re.sub(r'\s+', ' ', content).strip()
        return content
    
    # é¢„è§ˆè¾“å‡ºå‰3ä¸ªæœ‰å†…å®¹çš„ç±»åˆ«
    preview_sections = [
        ("é¡¹ç›®èèµ„", "2. é¡¹ç›®èèµ„ä»‹ç» (åç»­éœ€åŠ ä¸Šé¡¹ç›®ç±»åˆ«å¹¶åˆ é™¤å‰ç¼€ï¼Œä¸è¦å†™æ®XXæŠ¥é“ï¼‰", "project"),
        ("åŸºé‡‘èèµ„", "3. åŸºé‡‘èèµ„ä»‹ç»", "fund"),
        ("Portfolio", "14. Our portfolio (è¿™é‡Œæ ‡çº¢çš„åœ¨å…¬ä¼—å·ç¼–è¾‘æ—¶å¯¹åº”æ ‡çº¢å³å¯)", "portfolio")
    ]
    
    for category_name, section_title, result_key in preview_sections:
        articles_list = filtered_results.get(result_key, [])
        if articles_list:
            print(f"\n# {section_title}")
            for i, article in enumerate(articles_list[:2], 1):
                title = article['title']
                url = article['url']
                content = clean_content_preview(article.get('content_text', ''))
                preview_content = content[:150] + "..." if len(content) > 150 else content
                
                if result_key == "portfolio":
                    mentioned = article.get('mentioned_projects', [])
                    mentioned_str = f" [{', '.join(mentioned)}]" if mentioned else ""
                    print(f"\n{i}.[{title}]({url}){mentioned_str}")
                else:
                    print(f"\n{i}.[{title}]({url})")
                print(f"{preview_content}")

if __name__ == "__main__":
    main()